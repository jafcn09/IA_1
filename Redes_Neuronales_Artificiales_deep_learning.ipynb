{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jafcn09/IA_1/blob/main/Redes_Neuronales_Artificiales_deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Laboratorio **\n"
      ],
      "metadata": {
        "id": "2ui7xJWBQbet"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhixM3J6z8lN"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# **Perceptrón**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### implementación del flujo de datos a travez de un perceptrón\n",
        "\n",
        "$\\hat{y} = g(w_0 + x^T w)$\n",
        "\n",
        "con función de activación sigmoide\n",
        "\n",
        "$g(z) = \\sigma(z) = \\frac{1}{1+e^{-z}}$"
      ],
      "metadata": {
        "id": "7hdo7942JevZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "np.random.seed(50)\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n"
      ],
      "metadata": {
        "id": "hE8xHzxiJeKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### funciones de activación con Tensorflow"
      ],
      "metadata": {
        "id": "_zoIdbWjPjAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "#tf.math.sigmoid(z)\n",
        "#tf.math.tanh(z)\n",
        "#tf.math.relu(z)\n"
      ],
      "metadata": {
        "id": "eBdFrrZbPiNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio: implementar las funciones de activación tangente hiperbólico, y RELU"
      ],
      "metadata": {
        "id": "qojsnzffM-Ur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementación usando sklearn"
      ],
      "metadata": {
        "id": "pKFOwlYMJpsa"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z8WFguLE2ub",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "outputId": "3a5975d7-cd1a-40ea-8a6e-1a4e83b07e4a"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "#(iris.target == 0).astype(np.int)  # Iris setosa?\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
        "\n",
        "clf = Perceptron()\n",
        "# equivalente = SGDClassifier(loss=\"perceptron\", eta0=1, learning_rate=\"constant\", penalty=None).\n",
        "clf.fit(X, y)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred, labels=clf.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=clf.classes_)\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "# validación de modelo\n",
        "scores_cv = cross_val_score(clf, X_train, y_train, cv=10, scoring='accuracy')\n",
        "print(\"Error de validación cruzada =10,  Acc\", scores_cv.mean(), scores_cv.std())\n",
        "\n",
        "# prediction error on test set\n",
        "print(\"Error de predicción en el conjunto de test\", accuracy_score(y_test, y_pred))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error de validación cruzada =10,  Acc 0.7772727272727273 0.1459665861106223\n",
            "Error de predicción en el conjunto de test 0.6052631578947368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### El perceptrón con tensorflow"
      ],
      "metadata": {
        "id": "_p5kMDMZw8C5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Conjunto de datos\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n"
      ],
      "metadata": {
        "id": "973q3JIV6J4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# paso 1: crear la arquitectura usando la API sequential\n",
        "model = keras.Sequential([layers.Dense(units=1, input_shape=[4])])\n",
        "\n",
        "# paso 2: compilar el modelo\n",
        "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-jQf3Ydw8Rt",
        "outputId": "eeb01383-df41-46c8-8ec0-8ac05a1c180c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5 (20.00 Byte)\n",
            "Trainable params: 5 (20.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#paso 3: entrenando la red\n",
        "model.fit(X_train, y_train, epochs=225, batch_size=32, verbose=1, validation_split=0.2)"
      ],
      "metadata": {
        "id": "p9w6ArrxymfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#paso 4: evaluar el modelo\n",
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beCviSwbGBoh",
        "outputId": "507803d0-01f3-4947-e1ed-3b58cf4e498f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 5ms/step - loss: 1.6052 - accuracy: 0.4211\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.6051830053329468, 0.42105263471603394]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **El perceptrón Multicapa (MLP)**"
      ],
      "metadata": {
        "id": "TU16_5PFR_Nv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Implementación de clasificación supervisada con TensorFlow**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "hu_7MVCwapl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Keras version:\", keras.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxzNnG7ork5N",
        "outputId": "89e7d5cf-db1b-42cf-996c-997da9cf4ac0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.8.2\n",
            "Keras version: 2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "qEv0C7Rcr1YJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28140622-f0e5-4632-fcd4-17cc5401bdf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_full.shape)\n",
        "print(X_train_full.dtype)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwZ__L3Sr5Bf",
        "outputId": "a4a309ad-d54f-422b-b3dc-d1b8f7b2ff4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "uint8\n",
            "(10000, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "idx = 15000\n",
        "print(y_train_full[idx])\n",
        "plt.imshow(X_train_full[idx]) # add cmap='gray' for grayscale image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "BgadIzHYTjv-",
        "outputId": "caab1858-dc03-448f-9e7f-4806d8fa8dda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f0fe266c350>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
        "\n",
        "#   T-shirt/top -> [1,0,0,0,0,0,0,0,0,0]\n",
        "#   Trouser -> [0,1,0,0,0,0,0,0,0,0]\n",
        "#   ....\n",
        "#   Ankle boot -> [0,0,0,0,0,0,0,0,0,1]\n",
        "\n",
        "#class_names = ['Polo/Top', 'Pantalon', 'Suéter', 'Vestido','Casaca', 'Sandálias','Camisas', 'Zapatilla', 'Bolsa', 'Botas']"
      ],
      "metadata": {
        "id": "ExB9yq55sap3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,6))\n",
        "\n",
        "for i in range(25):\n",
        "    plt.subplot(5, 5, i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(X_train_full[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[y_train_full[i]])\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "PmeqE2FCnF5G",
        "outputId": "3fc75262-971b-4edd-d488-13b8c21081e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 25 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# para entrenar una red neuronal, siempre escalar/estandarizar las entradas\n",
        "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "X_test = X_test / 255.0"
      ],
      "metadata": {
        "id": "qTZxLH-RsBWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[28, 28])) # capa de entrada\n",
        "model.add(keras.layers.Dense(300, activation=\"relu\")) # primera capa oculta\n",
        "model.add(keras.layers.Dense(100, activation=\"relu\")) # segunda capa oculta\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\")) # capa de salida"
      ],
      "metadata": {
        "id": "BwM_MbR3shS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "alternativamente:"
      ],
      "metadata": {
        "id": "cCiB_4JGUbRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\" ),\n",
        "    keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# 235500 = 784*300 + 300\n",
        "# 30100  = 300*100 + 100\n",
        "# 1010   = 100*10 + 10\n",
        "\n",
        "# parametros de la red: pesos (w) y bias (w_0)\n",
        "# hiperparametros: # de neuronas, # de capas, funciones de activacion, # algoritmo de optimizacion, #learning rate"
      ],
      "metadata": {
        "id": "Wbc8x_Eysl9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qkHLQxns2VI",
        "outputId": "e0c1966f-87f8-447b-a9b8-c0e6852fa667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 300)               235500    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 100)               30100     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 266,610\n",
            "Trainable params: 266,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ns8QtWsas7FW",
        "outputId": "256f6f8f-3e0c-42e7-b6e8-7763a4d5188e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.layers.core.flatten.Flatten at 0x7fc8a6e91850>,\n",
              " <keras.layers.core.dense.Dense at 0x7fc8a6e912d0>,\n",
              " <keras.layers.core.dense.Dense at 0x7fc8a9fd2a90>,\n",
              " <keras.layers.core.dense.Dense at 0x7fc8a9fdc710>]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights, biases = model.layers[1].get_weights()\n",
        "weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0m_4VH3_tEcg",
        "outputId": "88bee0e6-0226-4623-c91f-2bee819ec493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.023713  ,  0.01772482, -0.0798324 , ...,  0.03531925,\n",
              "        -0.04475737,  0.04981199],\n",
              "       [-0.04301819,  0.00605654,  0.04986339, ...,  0.04219566,\n",
              "        -0.05928753, -0.00518456],\n",
              "       [ 0.03130274,  0.05029012,  0.01638827, ..., -0.03402678,\n",
              "        -0.05311136,  0.09867454],\n",
              "       ...,\n",
              "       [ 0.0148634 , -0.00736075, -0.02513357, ...,  0.06550874,\n",
              "        -0.00714925,  0.02740023],\n",
              "       [-0.03905753,  0.03581209,  0.01642968, ..., -0.01718093,\n",
              "         0.06807566,  0.02596275],\n",
              "       [ 0.07660411, -0.05282706,  0.00858237, ..., -0.09737465,\n",
              "        -0.01987456, -0.02708251]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwwDjFNjtNoy",
        "outputId": "6624683f-9f0f-4d94-dbc8-170cd7378083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(784, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "biases"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CX2GM7yHtPU3",
        "outputId": "cf24c993-a8c7-43ca-b674-5c620979316f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "biases.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3Kr74MKtRxp",
        "outputId": "20ed5105-1e13-4576-8570-72e7cb5b24da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss = \"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# sparse_categorical_crossentropy vs categorical_crossentropy son la misma loss, pero en 'sparse_categorical_crossentropy' la codificación del target son enteros como en este ejemplo, i.e., clase[0-9].\n",
        "# Usar categorical_crossentropy si la codificación del target es 'one-hot'"
      ],
      "metadata": {
        "id": "k-wtJQLTtUym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))\n",
        "#history.history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucldTGNgtb-h",
        "outputId": "10214aaf-d07b-4a48-a94e-b27feefdd4c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1719/1719 [==============================] - 14s 8ms/step - loss: 0.7087 - accuracy: 0.7662 - val_loss: 0.4990 - val_accuracy: 0.8306\n",
            "Epoch 2/30\n",
            "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4849 - accuracy: 0.8312 - val_loss: 0.4438 - val_accuracy: 0.8508\n",
            "Epoch 3/30\n",
            "1719/1719 [==============================] - 8s 4ms/step - loss: 0.4400 - accuracy: 0.8455 - val_loss: 0.4049 - val_accuracy: 0.8654\n",
            "Epoch 4/30\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4127 - accuracy: 0.8543 - val_loss: 0.3952 - val_accuracy: 0.8646\n",
            "Epoch 5/30\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3910 - accuracy: 0.8622 - val_loss: 0.3790 - val_accuracy: 0.8688\n",
            "Epoch 6/30\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3760 - accuracy: 0.8669 - val_loss: 0.3692 - val_accuracy: 0.8690\n",
            "Epoch 7/30\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3627 - accuracy: 0.8709 - val_loss: 0.3563 - val_accuracy: 0.8764\n",
            "Epoch 8/30\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 0.3515 - accuracy: 0.8738 - val_loss: 0.3924 - val_accuracy: 0.8620\n",
            "Epoch 9/30\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3404 - accuracy: 0.8789 - val_loss: 0.3541 - val_accuracy: 0.8744\n",
            "Epoch 10/30\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 0.3319 - accuracy: 0.8811 - val_loss: 0.3419 - val_accuracy: 0.8800\n",
            "Epoch 11/30\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3231 - accuracy: 0.8837 - val_loss: 0.3356 - val_accuracy: 0.8834\n",
            "Epoch 12/30\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 0.3143 - accuracy: 0.8877 - val_loss: 0.3296 - val_accuracy: 0.8820\n",
            "Epoch 13/30\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 0.3064 - accuracy: 0.8887 - val_loss: 0.3466 - val_accuracy: 0.8778\n",
            "Epoch 14/30\n",
            "1719/1719 [==============================] - 8s 4ms/step - loss: 0.3015 - accuracy: 0.8899 - val_loss: 0.3192 - val_accuracy: 0.8896\n",
            "Epoch 15/30\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2951 - accuracy: 0.8937 - val_loss: 0.3161 - val_accuracy: 0.8876\n",
            "Epoch 16/30\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2885 - accuracy: 0.8971 - val_loss: 0.3126 - val_accuracy: 0.8888\n",
            "Epoch 17/30\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2825 - accuracy: 0.8978 - val_loss: 0.3347 - val_accuracy: 0.8812\n",
            "Epoch 18/30\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2773 - accuracy: 0.8994 - val_loss: 0.3168 - val_accuracy: 0.8878\n",
            "Epoch 19/30\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2715 - accuracy: 0.9022 - val_loss: 0.3227 - val_accuracy: 0.8852\n",
            "Epoch 20/30\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2675 - accuracy: 0.9036 - val_loss: 0.3132 - val_accuracy: 0.8892\n",
            "Epoch 21/30\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2619 - accuracy: 0.9057 - val_loss: 0.3090 - val_accuracy: 0.8866\n",
            "Epoch 22/30\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2585 - accuracy: 0.9069 - val_loss: 0.3115 - val_accuracy: 0.8878\n",
            "Epoch 23/30\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2529 - accuracy: 0.9082 - val_loss: 0.3052 - val_accuracy: 0.8916\n",
            "Epoch 24/30\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2477 - accuracy: 0.9106 - val_loss: 0.3168 - val_accuracy: 0.8902\n",
            "Epoch 25/30\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2446 - accuracy: 0.9121 - val_loss: 0.3189 - val_accuracy: 0.8876\n",
            "Epoch 26/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2409 - accuracy: 0.9123 - val_loss: 0.3000 - val_accuracy: 0.8950\n",
            "Epoch 27/30\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2367 - accuracy: 0.9135 - val_loss: 0.2971 - val_accuracy: 0.8962\n",
            "Epoch 28/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2323 - accuracy: 0.9162 - val_loss: 0.3057 - val_accuracy: 0.8936\n",
            "Epoch 29/30\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2279 - accuracy: 0.9176 - val_loss: 0.3222 - val_accuracy: 0.8838\n",
            "Epoch 30/30\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2242 - accuracy: 0.9185 - val_loss: 0.2926 - val_accuracy: 0.8984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "H48wkWKmtna2",
        "outputId": "4351e795-406b-4c69-8fa5-25452e1a3aeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImFf2SzZtrQt",
        "outputId": "9cd40277-a9bc-4239-acc9-100e1a972512"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3287 - accuracy: 0.8835\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.32868605852127075, 0.8834999799728394]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### predicciones\n"
      ],
      "metadata": {
        "id": "4gE7lZ1HPSpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_new = X_test[:3]\n",
        "y_proba = model.predict(X_new)\n",
        "y_proba.round(2)\n",
        "\n",
        "#class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "#               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xew82-3jYtqV",
        "outputId": "f5a85768-cb85-4f11-bc04-abfd761e2f3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99],\n",
              "       [0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
              "       [0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=np.argmax(model.predict(X_new) ,axis=1)\n",
        "print(y_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c537lB83Y4WO",
        "outputId": "965d6342-2528-42a0-eea1-96ad5528fde6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9 2 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Implementación de regresión  con TensorFlow**"
      ],
      "metadata": {
        "id": "6nJLOFNqZIax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_valid = scaler.transform(X_valid)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "KvcC0Wu-ZPMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# arquitectura\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# compile\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
        "\n",
        "# entrenamiento\n",
        "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
        "\n",
        "# evaluar en el conjunto de test\n",
        "mse_test = model.evaluate(X_test, y_test)\n",
        "\n",
        "# realizar algunas predicciones\n",
        "X_new = X_test[:3]\n",
        "y_pred = model.predict(X_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "homhAwW6ZXbf",
        "outputId": "0a7dce2d-5d5d-4a10-f3c5-54480c9898b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "363/363 [==============================] - 2s 3ms/step - loss: 0.8141 - val_loss: 0.5349\n",
            "Epoch 2/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5487 - val_loss: 0.4678\n",
            "Epoch 3/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4991 - val_loss: 0.4470\n",
            "Epoch 4/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4677 - val_loss: 0.4313\n",
            "Epoch 5/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4554 - val_loss: 0.4226\n",
            "Epoch 6/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4460 - val_loss: 0.4190\n",
            "Epoch 7/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4447 - val_loss: 0.4089\n",
            "Epoch 8/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4539 - val_loss: 0.3996\n",
            "Epoch 9/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4370 - val_loss: 0.3976\n",
            "Epoch 10/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4237 - val_loss: 0.3908\n",
            "Epoch 11/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4212 - val_loss: 0.3835\n",
            "Epoch 12/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4189 - val_loss: 0.3883\n",
            "Epoch 13/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4104 - val_loss: 0.3765\n",
            "Epoch 14/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4043 - val_loss: 0.3718\n",
            "Epoch 15/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4010 - val_loss: 0.3695\n",
            "Epoch 16/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3975 - val_loss: 0.3664\n",
            "Epoch 17/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3937 - val_loss: 0.3639\n",
            "Epoch 18/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3938 - val_loss: 0.3650\n",
            "Epoch 19/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3878 - val_loss: 0.3583\n",
            "Epoch 20/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3856 - val_loss: 0.3580\n",
            "162/162 [==============================] - 0s 1ms/step - loss: 0.3827\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"my_keras_model.h5\")"
      ],
      "metadata": {
        "id": "xMZEBxICau7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(\"my_keras_model.h5\")"
      ],
      "metadata": {
        "id": "hL85ij6dchtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **regularización con early stopping**"
      ],
      "metadata": {
        "id": "NDHhXbHMdKzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
        "    keras.layers.Dense(30, activation=\"relu\"),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
        "\n",
        "#------callbacks\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\",\n",
        "                                                save_best_only=True)\n",
        "\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
        "                                                  restore_best_weights=True)\n",
        "\n",
        "#---------------\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=250,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[checkpoint_cb,early_stopping_cb])\n",
        "\n",
        "#model = keras.models.load_model(\"my_keras_model.h5\") # roll back to best model\n",
        "\n",
        "\n",
        "mse_test = model.evaluate(X_test, y_test)\n",
        "X_new = X_test[:3]\n",
        "y_pred = model.predict(X_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhBqE-HvdMt_",
        "outputId": "f6fb6dc0-cf24-433a-e8b9-8e02fd249066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "363/363 [==============================] - 2s 3ms/step - loss: 1.0079 - val_loss: 0.4855\n",
            "Epoch 2/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.7066 - val_loss: 0.4740\n",
            "Epoch 3/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4666 - val_loss: 0.4221\n",
            "Epoch 4/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4400 - val_loss: 0.4012\n",
            "Epoch 5/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4252 - val_loss: 0.3867\n",
            "Epoch 6/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4113 - val_loss: 0.3809\n",
            "Epoch 7/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4018 - val_loss: 0.3674\n",
            "Epoch 8/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3921 - val_loss: 0.3609\n",
            "Epoch 9/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3861 - val_loss: 0.3547\n",
            "Epoch 10/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3810 - val_loss: 0.3490\n",
            "Epoch 11/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3745 - val_loss: 0.3454\n",
            "Epoch 12/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3685 - val_loss: 0.3422\n",
            "Epoch 13/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3679 - val_loss: 0.3354\n",
            "Epoch 14/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3603 - val_loss: 0.3352\n",
            "Epoch 15/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3562 - val_loss: 0.3312\n",
            "Epoch 16/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3514 - val_loss: 0.3391\n",
            "Epoch 17/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3476 - val_loss: 0.3230\n",
            "Epoch 18/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3453 - val_loss: 0.3164\n",
            "Epoch 19/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3433 - val_loss: 0.3229\n",
            "Epoch 20/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3423 - val_loss: 0.3115\n",
            "Epoch 21/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3361 - val_loss: 0.3122\n",
            "Epoch 22/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3345 - val_loss: 0.3098\n",
            "Epoch 23/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3318 - val_loss: 0.3062\n",
            "Epoch 24/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3287 - val_loss: 0.3046\n",
            "Epoch 25/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3264 - val_loss: 0.3021\n",
            "Epoch 26/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3272 - val_loss: 0.2991\n",
            "Epoch 27/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3235 - val_loss: 0.3058\n",
            "Epoch 28/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3222 - val_loss: 0.2958\n",
            "Epoch 29/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3180 - val_loss: 0.3079\n",
            "Epoch 30/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3174 - val_loss: 0.2894\n",
            "Epoch 31/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3145 - val_loss: 0.2929\n",
            "Epoch 32/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3127 - val_loss: 0.2895\n",
            "Epoch 33/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3106 - val_loss: 0.2961\n",
            "Epoch 34/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3093 - val_loss: 0.2869\n",
            "Epoch 35/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3067 - val_loss: 0.2899\n",
            "Epoch 36/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3065 - val_loss: 0.2964\n",
            "Epoch 37/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3043 - val_loss: 0.2783\n",
            "Epoch 38/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3033 - val_loss: 0.2971\n",
            "Epoch 39/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3023 - val_loss: 0.2919\n",
            "Epoch 40/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3012 - val_loss: 0.2878\n",
            "Epoch 41/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2970 - val_loss: 0.2754\n",
            "Epoch 42/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2970 - val_loss: 0.2715\n",
            "Epoch 43/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2965 - val_loss: 0.2716\n",
            "Epoch 44/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2962 - val_loss: 0.3177\n",
            "Epoch 45/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2935 - val_loss: 0.2770\n",
            "Epoch 46/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2955 - val_loss: 0.2810\n",
            "Epoch 47/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2940 - val_loss: 0.2720\n",
            "Epoch 48/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2940 - val_loss: 0.2753\n",
            "Epoch 49/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2915 - val_loss: 0.2709\n",
            "Epoch 50/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2925 - val_loss: 0.2786\n",
            "Epoch 51/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2932 - val_loss: 0.2777\n",
            "Epoch 52/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2914 - val_loss: 0.2804\n",
            "Epoch 53/250\n",
            "363/363 [==============================] - 1s 3ms/step - loss: 0.2902 - val_loss: 0.2687\n",
            "Epoch 54/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2893 - val_loss: 0.2673\n",
            "Epoch 55/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2893 - val_loss: 0.2897\n",
            "Epoch 56/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2872 - val_loss: 0.3107\n",
            "Epoch 57/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2863 - val_loss: 0.2792\n",
            "Epoch 58/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2872 - val_loss: 0.2663\n",
            "Epoch 59/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2853 - val_loss: 0.2646\n",
            "Epoch 60/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2854 - val_loss: 0.2869\n",
            "Epoch 61/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2851 - val_loss: 0.2737\n",
            "Epoch 62/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2817 - val_loss: 0.2759\n",
            "Epoch 63/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2831 - val_loss: 0.2670\n",
            "Epoch 64/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2825 - val_loss: 0.2724\n",
            "Epoch 65/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2827 - val_loss: 0.2610\n",
            "Epoch 66/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2814 - val_loss: 0.2812\n",
            "Epoch 67/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2821 - val_loss: 0.2624\n",
            "Epoch 68/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2827 - val_loss: 0.2672\n",
            "Epoch 69/250\n",
            "363/363 [==============================] - 2s 4ms/step - loss: 0.2808 - val_loss: 0.2831\n",
            "Epoch 70/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2788 - val_loss: 0.3009\n",
            "Epoch 71/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2790 - val_loss: 0.2686\n",
            "Epoch 72/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2780 - val_loss: 0.2863\n",
            "Epoch 73/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2781 - val_loss: 0.2687\n",
            "Epoch 74/250\n",
            "363/363 [==============================] - 1s 3ms/step - loss: 0.2798 - val_loss: 0.2607\n",
            "Epoch 75/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2784 - val_loss: 0.2821\n",
            "Epoch 76/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2761 - val_loss: 0.2741\n",
            "Epoch 77/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2772 - val_loss: 0.2708\n",
            "Epoch 78/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2766 - val_loss: 0.2625\n",
            "Epoch 79/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2761 - val_loss: 0.2679\n",
            "Epoch 80/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2745 - val_loss: 0.2795\n",
            "Epoch 81/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2769 - val_loss: 0.2694\n",
            "Epoch 82/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2735 - val_loss: 0.2636\n",
            "Epoch 83/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2754 - val_loss: 0.2646\n",
            "Epoch 84/250\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2734 - val_loss: 0.2661\n",
            "162/162 [==============================] - 0s 2ms/step - loss: 0.2898\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Selección de modelo con Sklearn & Tensorflow**\n",
        "\n"
      ],
      "metadata": {
        "id": "FgRB5h2cicF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
        "    for layer in range(n_hidden):\n",
        "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
        "    model.add(keras.layers.Dense(1))\n",
        "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
        "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "    return model"
      ],
      "metadata": {
        "id": "XibTHsVNiino"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPmVEhfHirUS",
        "outputId": "3f8d41e2-2038-4809-a5a9-4ebe6a2a112c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keras_reg.fit(X_train, y_train, epochs=100,\n",
        "              validation_data=(X_valid, y_valid),\n",
        "              callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
        "mse_test = keras_reg.score(X_test, y_test)\n",
        "y_pred = keras_reg.predict(X_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXEwTNeuitRo",
        "outputId": "a6d5b615-aad1-4345-be00-e7761866cd2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "363/363 [==============================] - 1s 2ms/step - loss: 1.0834 - val_loss: 0.7352\n",
            "Epoch 2/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.6617 - val_loss: 0.6113\n",
            "Epoch 3/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5664 - val_loss: 0.5652\n",
            "Epoch 4/100\n",
            "363/363 [==============================] - 1s 3ms/step - loss: 0.5301 - val_loss: 0.5374\n",
            "Epoch 5/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5073 - val_loss: 0.5167\n",
            "Epoch 6/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4907 - val_loss: 0.5039\n",
            "Epoch 7/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4806 - val_loss: 0.4973\n",
            "Epoch 8/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4710 - val_loss: 0.4861\n",
            "Epoch 9/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4631 - val_loss: 0.4813\n",
            "Epoch 10/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4585 - val_loss: 0.4754\n",
            "Epoch 11/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4529 - val_loss: 0.4701\n",
            "Epoch 12/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4484 - val_loss: 0.4661\n",
            "Epoch 13/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4432 - val_loss: 0.4619\n",
            "Epoch 14/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4420 - val_loss: 0.4596\n",
            "Epoch 15/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4375 - val_loss: 0.4564\n",
            "Epoch 16/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4352 - val_loss: 0.4532\n",
            "Epoch 17/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4315 - val_loss: 0.4525\n",
            "Epoch 18/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4295 - val_loss: 0.4499\n",
            "Epoch 19/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4272 - val_loss: 0.4470\n",
            "Epoch 20/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4252 - val_loss: 0.4490\n",
            "Epoch 21/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4229 - val_loss: 0.4438\n",
            "Epoch 22/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4206 - val_loss: 0.4475\n",
            "Epoch 23/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4184 - val_loss: 0.4398\n",
            "Epoch 24/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4169 - val_loss: 0.4396\n",
            "Epoch 25/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4155 - val_loss: 0.4348\n",
            "Epoch 26/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4120 - val_loss: 0.4329\n",
            "Epoch 27/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4097 - val_loss: 0.4319\n",
            "Epoch 28/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4077 - val_loss: 0.4287\n",
            "Epoch 29/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4058 - val_loss: 0.4276\n",
            "Epoch 30/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4037 - val_loss: 0.4242\n",
            "Epoch 31/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4015 - val_loss: 0.4254\n",
            "Epoch 32/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4005 - val_loss: 0.4208\n",
            "Epoch 33/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4008 - val_loss: 0.4225\n",
            "Epoch 34/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4016 - val_loss: 0.4193\n",
            "Epoch 35/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3963 - val_loss: 0.4190\n",
            "Epoch 36/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3937 - val_loss: 0.4212\n",
            "Epoch 37/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3927 - val_loss: 0.4164\n",
            "Epoch 38/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3948 - val_loss: 0.4156\n",
            "Epoch 39/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3900 - val_loss: 0.4190\n",
            "Epoch 40/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3906 - val_loss: 0.4139\n",
            "Epoch 41/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3908 - val_loss: 0.4111\n",
            "Epoch 42/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3862 - val_loss: 0.4104\n",
            "Epoch 43/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3845 - val_loss: 0.4089\n",
            "Epoch 44/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3836 - val_loss: 0.4071\n",
            "Epoch 45/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3839 - val_loss: 0.4086\n",
            "Epoch 46/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3827 - val_loss: 0.4054\n",
            "Epoch 47/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3815 - val_loss: 0.4057\n",
            "Epoch 48/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3798 - val_loss: 0.4036\n",
            "Epoch 49/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3781 - val_loss: 0.4040\n",
            "Epoch 50/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3774 - val_loss: 0.4018\n",
            "Epoch 51/100\n",
            "363/363 [==============================] - 1s 3ms/step - loss: 0.3767 - val_loss: 0.4068\n",
            "Epoch 52/100\n",
            "363/363 [==============================] - 1s 3ms/step - loss: 0.3762 - val_loss: 0.3997\n",
            "Epoch 53/100\n",
            "363/363 [==============================] - 1s 3ms/step - loss: 0.3749 - val_loss: 0.4013\n",
            "Epoch 54/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3743 - val_loss: 0.4008\n",
            "Epoch 55/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3744 - val_loss: 0.3995\n",
            "Epoch 56/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3727 - val_loss: 0.3980\n",
            "Epoch 57/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3722 - val_loss: 0.3965\n",
            "Epoch 58/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3707 - val_loss: 0.3957\n",
            "Epoch 59/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3694 - val_loss: 0.3974\n",
            "Epoch 60/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3700 - val_loss: 0.3954\n",
            "Epoch 61/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3688 - val_loss: 0.3973\n",
            "Epoch 62/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3681 - val_loss: 0.3955\n",
            "Epoch 63/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3691 - val_loss: 0.3981\n",
            "Epoch 64/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3718 - val_loss: 0.3928\n",
            "Epoch 65/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3665 - val_loss: 0.3956\n",
            "Epoch 66/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3689 - val_loss: 0.3943\n",
            "Epoch 67/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3667 - val_loss: 0.3933\n",
            "Epoch 68/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3667 - val_loss: 0.3907\n",
            "Epoch 69/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3635 - val_loss: 0.3924\n",
            "Epoch 70/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3646 - val_loss: 0.3885\n",
            "Epoch 71/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3620 - val_loss: 0.3892\n",
            "Epoch 72/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3627 - val_loss: 0.3950\n",
            "Epoch 73/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3624 - val_loss: 0.3923\n",
            "Epoch 74/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3604 - val_loss: 0.3864\n",
            "Epoch 75/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3596 - val_loss: 0.3862\n",
            "Epoch 76/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3594 - val_loss: 0.3870\n",
            "Epoch 77/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3597 - val_loss: 0.3872\n",
            "Epoch 78/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3607 - val_loss: 0.3894\n",
            "Epoch 79/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3612 - val_loss: 0.3941\n",
            "Epoch 80/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3692 - val_loss: 0.3959\n",
            "Epoch 81/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3680 - val_loss: 0.3825\n",
            "Epoch 82/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3580 - val_loss: 0.3837\n",
            "Epoch 83/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3594 - val_loss: 0.3826\n",
            "Epoch 84/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3557 - val_loss: 0.3829\n",
            "Epoch 85/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3583 - val_loss: 0.3806\n",
            "Epoch 86/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3547 - val_loss: 0.3847\n",
            "Epoch 87/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3560 - val_loss: 0.3828\n",
            "Epoch 88/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3528 - val_loss: 0.3796\n",
            "Epoch 89/100\n",
            "363/363 [==============================] - 2s 5ms/step - loss: 0.3530 - val_loss: 0.3805\n",
            "Epoch 90/100\n",
            "363/363 [==============================] - 1s 4ms/step - loss: 0.3525 - val_loss: 0.3796\n",
            "Epoch 91/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3516 - val_loss: 0.3796\n",
            "Epoch 92/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3511 - val_loss: 0.3793\n",
            "Epoch 93/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3518 - val_loss: 0.3794\n",
            "Epoch 94/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3506 - val_loss: 0.3767\n",
            "Epoch 95/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3499 - val_loss: 0.3768\n",
            "Epoch 96/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3488 - val_loss: 0.3750\n",
            "Epoch 97/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3479 - val_loss: 0.3744\n",
            "Epoch 98/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3469 - val_loss: 0.3749\n",
            "Epoch 99/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3470 - val_loss: 0.3746\n",
            "Epoch 100/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3459 - val_loss: 0.3740\n",
            "162/162 [==============================] - 0s 1ms/step - loss: 0.3557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model selection\n",
        "from scipy.stats import reciprocal\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_distribs = {\n",
        "    \"n_hidden\": [0, 1, 2, 3],\n",
        "    \"n_neurons\": np.arange(1, 100),\n",
        "    \"learning_rate\": reciprocal(3e-4, 3e-2),\n",
        "}\n",
        "\n",
        "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
        "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
        "                  validation_data=(X_valid, y_valid),\n",
        "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
        "\n",
        "# tarea calcular el tiempo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbVu5sY6izcr",
        "outputId": "64e76fff-da40-40e0-ac7c-3b7e8e9dc41e"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "242/242 [==============================] - 1s 3ms/step - loss: 3.5327 - val_loss: 2.3777\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.7453 - val_loss: 1.3760\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.1972 - val_loss: 1.1369\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.0268 - val_loss: 1.0053\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.9149 - val_loss: 0.9043\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.8317 - val_loss: 0.8285\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7720 - val_loss: 0.7745\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7306 - val_loss: 0.7374\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7028 - val_loss: 0.7120\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6840 - val_loss: 0.6944\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6706 - val_loss: 0.6812\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6601 - val_loss: 0.6708\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6515 - val_loss: 0.6622\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6438 - val_loss: 0.6544\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6365 - val_loss: 0.6474\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6296 - val_loss: 0.6407\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6229 - val_loss: 0.6341\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6165 - val_loss: 0.6278\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6102 - val_loss: 0.6219\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6043 - val_loss: 0.6165\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5990 - val_loss: 0.6114\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5940 - val_loss: 0.6067\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.5897 - val_loss: 0.6023\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.5856 - val_loss: 0.5981\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5818 - val_loss: 0.5942\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5781 - val_loss: 0.5907\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5749 - val_loss: 0.5874\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5719 - val_loss: 0.5842\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5691 - val_loss: 0.5813\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5664 - val_loss: 0.5784\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5639 - val_loss: 0.5758\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5615 - val_loss: 0.5732\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5591 - val_loss: 0.5707\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5569 - val_loss: 0.5684\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5547 - val_loss: 0.5660\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5528 - val_loss: 0.5638\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5507 - val_loss: 0.5616\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5486 - val_loss: 0.5597\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5466 - val_loss: 0.5575\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5555\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5430 - val_loss: 0.5535\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.5411 - val_loss: 0.5515\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.5391 - val_loss: 0.5498\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5377 - val_loss: 0.5479\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5360 - val_loss: 0.5461\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5344 - val_loss: 0.5442\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5327 - val_loss: 0.5424\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5310 - val_loss: 0.5405\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5293 - val_loss: 0.5388\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5275 - val_loss: 0.5370\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5259 - val_loss: 0.5352\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5243 - val_loss: 0.5335\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5227 - val_loss: 0.5318\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5211 - val_loss: 0.5302\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5195 - val_loss: 0.5286\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5179 - val_loss: 0.5268\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5163 - val_loss: 0.5253\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5147 - val_loss: 0.5237\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5132 - val_loss: 0.5221\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5118 - val_loss: 0.5207\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5103 - val_loss: 0.5192\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5088 - val_loss: 0.5177\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5072 - val_loss: 0.5162\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5056 - val_loss: 0.5147\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5043 - val_loss: 0.5133\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5028 - val_loss: 0.5119\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5013 - val_loss: 0.5106\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4999 - val_loss: 0.5092\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4985 - val_loss: 0.5080\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4972 - val_loss: 0.5070\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4959 - val_loss: 0.5057\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4947 - val_loss: 0.5044\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4935 - val_loss: 0.5032\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4923 - val_loss: 0.5022\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4912 - val_loss: 0.5009\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4901 - val_loss: 0.5000\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4890 - val_loss: 0.4988\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4880 - val_loss: 0.4979\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4869 - val_loss: 0.4970\n",
            "Epoch 80/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4861 - val_loss: 0.4957\n",
            "Epoch 81/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4850 - val_loss: 0.4951\n",
            "Epoch 82/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4844 - val_loss: 0.4944\n",
            "Epoch 83/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4839 - val_loss: 0.4937\n",
            "Epoch 84/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4834 - val_loss: 0.4932\n",
            "Epoch 85/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4830 - val_loss: 0.4930\n",
            "Epoch 86/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4827 - val_loss: 0.4925\n",
            "Epoch 87/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4824 - val_loss: 0.4923\n",
            "Epoch 88/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4820 - val_loss: 0.4924\n",
            "Epoch 89/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4816 - val_loss: 0.4923\n",
            "Epoch 90/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4814 - val_loss: 0.4920\n",
            "Epoch 91/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4810 - val_loss: 0.4917\n",
            "Epoch 92/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4807 - val_loss: 0.4916\n",
            "Epoch 93/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4804 - val_loss: 0.4911\n",
            "Epoch 94/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4800 - val_loss: 0.4906\n",
            "Epoch 95/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4799 - val_loss: 0.4903\n",
            "Epoch 96/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4795 - val_loss: 0.4903\n",
            "Epoch 97/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4791 - val_loss: 0.4900\n",
            "Epoch 98/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4790 - val_loss: 0.4897\n",
            "Epoch 99/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4786 - val_loss: 0.4895\n",
            "Epoch 100/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4784 - val_loss: 0.4896\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 0.4628\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "242/242 [==============================] - 1s 3ms/step - loss: 4.7915 - val_loss: 4.0511\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 3.5105 - val_loss: 3.0596\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 2.7191 - val_loss: 2.4323\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 2.2178 - val_loss: 2.0338\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.8990 - val_loss: 1.7788\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.6958 - val_loss: 1.6166\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.5669 - val_loss: 1.5132\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.4848 - val_loss: 1.4470\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.4326 - val_loss: 1.4048\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 1.3993 - val_loss: 1.3778\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.3782 - val_loss: 1.3603\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.3646 - val_loss: 1.3491\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.3560 - val_loss: 1.3419\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.3505 - val_loss: 1.3373\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 1.3470 - val_loss: 1.3343\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.3449 - val_loss: 1.3323\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.3434 - val_loss: 1.3311\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.3425 - val_loss: 1.3302\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.3419 - val_loss: 1.3296\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.3415 - val_loss: 1.3292\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.3412 - val_loss: 1.3289\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 1.3410 - val_loss: 1.3287\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 1.3408 - val_loss: 1.3284\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.3405 - val_loss: 1.3280\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.3399 - val_loss: 1.3269\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.3380 - val_loss: 1.3236\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.3303 - val_loss: 1.3096\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.3116 - val_loss: 1.2919\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.2948 - val_loss: 1.2766\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.2797 - val_loss: 1.2622\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.2651 - val_loss: 1.2480\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.2503 - val_loss: 1.2336\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.2352 - val_loss: 1.2193\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.2206 - val_loss: 1.2054\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.2060 - val_loss: 1.1915\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.1914 - val_loss: 1.1774\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.1763 - val_loss: 1.1627\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.1606 - val_loss: 1.1473\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.1444 - val_loss: 1.1315\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.1276 - val_loss: 1.1151\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.1104 - val_loss: 1.0978\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 1.0929 - val_loss: 1.0803\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 1.0751 - val_loss: 1.0626\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.0571 - val_loss: 1.0447\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.0391 - val_loss: 1.0269\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.0211 - val_loss: 1.0092\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 1.0031 - val_loss: 0.9916\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.9854 - val_loss: 0.9746\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.9681 - val_loss: 0.9580\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.9515 - val_loss: 0.9420\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.9352 - val_loss: 0.9265\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.9195 - val_loss: 0.9116\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.9044 - val_loss: 0.8973\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.8900 - val_loss: 0.8836\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.8763 - val_loss: 0.8706\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.8632 - val_loss: 0.8583\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.8509 - val_loss: 0.8465\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.8390 - val_loss: 0.8352\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.8278 - val_loss: 0.8245\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.8172 - val_loss: 0.8144\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.8071 - val_loss: 0.8049\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7976 - val_loss: 0.7959\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7886 - val_loss: 0.7874\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7799 - val_loss: 0.7791\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7718 - val_loss: 0.7715\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7640 - val_loss: 0.7640\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7564 - val_loss: 0.7572\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7493 - val_loss: 0.7502\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7425 - val_loss: 0.7439\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7360 - val_loss: 0.7378\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7296 - val_loss: 0.7317\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7237 - val_loss: 0.7260\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7179 - val_loss: 0.7205\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7122 - val_loss: 0.7153\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7067 - val_loss: 0.7103\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7016 - val_loss: 0.7050\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6965 - val_loss: 0.7002\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6915 - val_loss: 0.6953\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6867 - val_loss: 0.6907\n",
            "Epoch 80/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6820 - val_loss: 0.6861\n",
            "Epoch 81/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6774 - val_loss: 0.6816\n",
            "Epoch 82/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6728 - val_loss: 0.6774\n",
            "Epoch 83/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6683 - val_loss: 0.6730\n",
            "Epoch 84/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6639 - val_loss: 0.6688\n",
            "Epoch 85/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6596 - val_loss: 0.6646\n",
            "Epoch 86/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6555 - val_loss: 0.6606\n",
            "Epoch 87/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6514 - val_loss: 0.6566\n",
            "Epoch 88/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6473 - val_loss: 0.6527\n",
            "Epoch 89/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6433 - val_loss: 0.6488\n",
            "Epoch 90/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6395 - val_loss: 0.6452\n",
            "Epoch 91/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6356 - val_loss: 0.6414\n",
            "Epoch 92/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6319 - val_loss: 0.6379\n",
            "Epoch 93/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6283 - val_loss: 0.6344\n",
            "Epoch 94/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6247 - val_loss: 0.6312\n",
            "Epoch 95/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6212 - val_loss: 0.6278\n",
            "Epoch 96/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6179 - val_loss: 0.6245\n",
            "Epoch 97/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6146 - val_loss: 0.6213\n",
            "Epoch 98/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6114 - val_loss: 0.6182\n",
            "Epoch 99/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6082 - val_loss: 0.6153\n",
            "Epoch 100/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6051 - val_loss: 0.6123\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 0.6092\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "242/242 [==============================] - 1s 4ms/step - loss: 4.1744 - val_loss: 3.1130\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 2.4784 - val_loss: 1.9704\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.7639 - val_loss: 1.6270\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.5339 - val_loss: 1.4744\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.4053 - val_loss: 1.3676\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.3119 - val_loss: 1.2885\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.2411 - val_loss: 1.2259\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.1856 - val_loss: 1.1781\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.1412 - val_loss: 1.1395\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.1050 - val_loss: 1.1085\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.0751 - val_loss: 1.0830\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.0500 - val_loss: 1.0604\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.0279 - val_loss: 1.0403\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.0082 - val_loss: 1.0226\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.9902 - val_loss: 1.0062\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.9733 - val_loss: 0.9912\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.9570 - val_loss: 0.9760\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.9412 - val_loss: 0.9618\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.9261 - val_loss: 0.9476\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.9113 - val_loss: 0.9337\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.8968 - val_loss: 0.9203\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.8828 - val_loss: 0.9074\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.8697 - val_loss: 0.8946\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.8573 - val_loss: 0.8822\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.8452 - val_loss: 0.8696\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.8333 - val_loss: 0.8575\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.8221 - val_loss: 0.8457\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.8114 - val_loss: 0.8340\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.8013 - val_loss: 0.8227\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7916 - val_loss: 0.8120\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7824 - val_loss: 0.8018\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7738 - val_loss: 0.7923\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7656 - val_loss: 0.7836\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7580 - val_loss: 0.7752\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7510 - val_loss: 0.7673\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7446 - val_loss: 0.7602\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7387 - val_loss: 0.7539\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7331 - val_loss: 0.7479\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7279 - val_loss: 0.7423\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7231 - val_loss: 0.7373\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7188 - val_loss: 0.7324\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.7147 - val_loss: 0.7277\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7108 - val_loss: 0.7236\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7074 - val_loss: 0.7204\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7046 - val_loss: 0.7171\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7019 - val_loss: 0.7143\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6995 - val_loss: 0.7115\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6969 - val_loss: 0.7090\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.6945 - val_loss: 0.7066\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6922 - val_loss: 0.7041\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6899 - val_loss: 0.7020\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6877 - val_loss: 0.7000\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6856 - val_loss: 0.6980\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6837 - val_loss: 0.6961\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6818 - val_loss: 0.6942\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6800 - val_loss: 0.6924\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6783 - val_loss: 0.6907\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6765 - val_loss: 0.6889\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6747 - val_loss: 0.6873\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6729 - val_loss: 0.6857\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.6714 - val_loss: 0.6838\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6697 - val_loss: 0.6822\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6680 - val_loss: 0.6805\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6663 - val_loss: 0.6789\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6646 - val_loss: 0.6772\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6630 - val_loss: 0.6756\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6613 - val_loss: 0.6739\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.6597 - val_loss: 0.6723\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.6581 - val_loss: 0.6707\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6564 - val_loss: 0.6691\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6548 - val_loss: 0.6676\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6532 - val_loss: 0.6661\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6516 - val_loss: 0.6645\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.6499 - val_loss: 0.6629\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.6482 - val_loss: 0.6614\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.6467 - val_loss: 0.6598\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6451 - val_loss: 0.6583\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.6435 - val_loss: 0.6568\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.6419 - val_loss: 0.6553\n",
            "Epoch 80/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6403 - val_loss: 0.6538\n",
            "Epoch 81/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.6387 - val_loss: 0.6523\n",
            "Epoch 82/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6371 - val_loss: 0.6507\n",
            "Epoch 83/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6355 - val_loss: 0.6491\n",
            "Epoch 84/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6339 - val_loss: 0.6474\n",
            "Epoch 85/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.6323 - val_loss: 0.6457\n",
            "Epoch 86/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6307 - val_loss: 0.6441\n",
            "Epoch 87/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.6289 - val_loss: 0.6425\n",
            "Epoch 88/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6272 - val_loss: 0.6408\n",
            "Epoch 89/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6256 - val_loss: 0.6389\n",
            "Epoch 90/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6236 - val_loss: 0.6370\n",
            "Epoch 91/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.6219 - val_loss: 0.6352\n",
            "Epoch 92/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.6200 - val_loss: 0.6333\n",
            "Epoch 93/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6182 - val_loss: 0.6315\n",
            "Epoch 94/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6163 - val_loss: 0.6297\n",
            "Epoch 95/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6145 - val_loss: 0.6278\n",
            "Epoch 96/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6126 - val_loss: 0.6259\n",
            "Epoch 97/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.6108 - val_loss: 0.6241\n",
            "Epoch 98/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6089 - val_loss: 0.6222\n",
            "Epoch 99/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6071 - val_loss: 0.6204\n",
            "Epoch 100/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.6052 - val_loss: 0.6186\n",
            "121/121 [==============================] - 0s 2ms/step - loss: 0.6212\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "242/242 [==============================] - 1s 3ms/step - loss: 1.3875 - val_loss: 0.7710\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6857 - val_loss: 0.6850\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.6267 - val_loss: 0.6303\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5848 - val_loss: 0.5912\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5553 - val_loss: 0.5614\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5323 - val_loss: 0.5362\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5159 - val_loss: 0.5206\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5021 - val_loss: 0.5075\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4915 - val_loss: 0.4966\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4822 - val_loss: 0.4921\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4743 - val_loss: 0.4864\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4675 - val_loss: 0.4820\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4625 - val_loss: 0.4802\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4575 - val_loss: 0.4797\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4530 - val_loss: 0.4789\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4487 - val_loss: 0.4814\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4458 - val_loss: 0.4788\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4428 - val_loss: 0.4812\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4390 - val_loss: 0.4816\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4367 - val_loss: 0.4810\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4342 - val_loss: 0.4837\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4319 - val_loss: 0.4846\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4295 - val_loss: 0.4879\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4270 - val_loss: 0.4898\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4251 - val_loss: 0.4937\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4225 - val_loss: 0.4938\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4209 - val_loss: 0.4962\n",
            "121/121 [==============================] - 0s 2ms/step - loss: 0.7305\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "242/242 [==============================] - 1s 3ms/step - loss: 1.9932 - val_loss: 0.7802\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.8161 - val_loss: 0.6689\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.6237 - val_loss: 0.6212\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.5836 - val_loss: 0.5852\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.5531 - val_loss: 0.5590\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.5293 - val_loss: 0.5400\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5115 - val_loss: 0.5210\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4981 - val_loss: 0.5143\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4855 - val_loss: 0.4991\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4793 - val_loss: 0.4966\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4721 - val_loss: 0.4867\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4669 - val_loss: 0.4801\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4613 - val_loss: 0.4863\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4568 - val_loss: 0.4752\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4542 - val_loss: 0.4708\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4488 - val_loss: 0.4676\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4492 - val_loss: 0.4647\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4457 - val_loss: 0.4630\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4404 - val_loss: 0.4597\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4380 - val_loss: 0.4588\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4350 - val_loss: 0.4546\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4335 - val_loss: 0.4551\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4307 - val_loss: 0.4500\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4288 - val_loss: 0.4493\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4259 - val_loss: 0.4470\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4243 - val_loss: 0.4466\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4221 - val_loss: 0.4441\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4198 - val_loss: 0.4420\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4176 - val_loss: 0.4400\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4164 - val_loss: 0.4385\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4151 - val_loss: 0.4399\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4126 - val_loss: 0.4352\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4113 - val_loss: 0.4341\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4094 - val_loss: 0.4333\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4077 - val_loss: 0.4316\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4059 - val_loss: 0.4304\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4044 - val_loss: 0.4301\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4039 - val_loss: 0.4283\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4015 - val_loss: 0.4275\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3999 - val_loss: 0.4254\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3988 - val_loss: 0.4258\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3984 - val_loss: 0.4242\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3959 - val_loss: 0.4240\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3958 - val_loss: 0.4227\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3941 - val_loss: 0.4213\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3931 - val_loss: 0.4199\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3912 - val_loss: 0.4195\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3899 - val_loss: 0.4188\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3886 - val_loss: 0.4178\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3871 - val_loss: 0.4192\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3868 - val_loss: 0.4171\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3862 - val_loss: 0.4176\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3861 - val_loss: 0.4150\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3834 - val_loss: 0.4124\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3819 - val_loss: 0.4128\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3810 - val_loss: 0.4121\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3809 - val_loss: 0.4105\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3804 - val_loss: 0.4104\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3781 - val_loss: 0.4091\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3780 - val_loss: 0.4087\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3755 - val_loss: 0.4108\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3774 - val_loss: 0.4086\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3742 - val_loss: 0.4078\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3787 - val_loss: 0.4058\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3722 - val_loss: 0.4079\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3719 - val_loss: 0.4043\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3706 - val_loss: 0.4081\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3697 - val_loss: 0.4054\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3705 - val_loss: 0.4018\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3689 - val_loss: 0.4051\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3679 - val_loss: 0.4008\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3671 - val_loss: 0.4003\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3661 - val_loss: 0.4005\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3660 - val_loss: 0.3995\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3662 - val_loss: 0.3981\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3643 - val_loss: 0.3985\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3632 - val_loss: 0.3971\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3623 - val_loss: 0.3977\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3627 - val_loss: 0.4115\n",
            "Epoch 80/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3729 - val_loss: 0.3956\n",
            "Epoch 81/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3626 - val_loss: 0.3965\n",
            "Epoch 82/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3607 - val_loss: 0.3952\n",
            "Epoch 83/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3595 - val_loss: 0.3949\n",
            "Epoch 84/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3578 - val_loss: 0.3931\n",
            "Epoch 85/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3586 - val_loss: 0.3928\n",
            "Epoch 86/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3571 - val_loss: 0.3956\n",
            "Epoch 87/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3563 - val_loss: 0.3909\n",
            "Epoch 88/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3555 - val_loss: 0.3935\n",
            "Epoch 89/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3548 - val_loss: 0.3919\n",
            "Epoch 90/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3545 - val_loss: 0.3946\n",
            "Epoch 91/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3543 - val_loss: 0.3906\n",
            "Epoch 92/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3541 - val_loss: 0.3878\n",
            "Epoch 93/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3531 - val_loss: 0.3883\n",
            "Epoch 94/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3525 - val_loss: 0.3876\n",
            "Epoch 95/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3551 - val_loss: 0.3890\n",
            "Epoch 96/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3513 - val_loss: 0.3908\n",
            "Epoch 97/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3513 - val_loss: 0.3872\n",
            "Epoch 98/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3523 - val_loss: 0.3858\n",
            "Epoch 99/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3511 - val_loss: 0.3853\n",
            "Epoch 100/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3498 - val_loss: 0.3846\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 0.3672\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "242/242 [==============================] - 1s 4ms/step - loss: 1.4097 - val_loss: 0.8295\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7904 - val_loss: 0.7016\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6522 - val_loss: 0.6323\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5940 - val_loss: 0.5945\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.5588 - val_loss: 0.5661\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.5338 - val_loss: 0.5456\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.5139 - val_loss: 0.5288\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5004 - val_loss: 0.5184\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4883 - val_loss: 0.5071\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4800 - val_loss: 0.5008\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4716 - val_loss: 0.4935\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4640 - val_loss: 0.4887\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4583 - val_loss: 0.4829\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4535 - val_loss: 0.4764\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4487 - val_loss: 0.4735\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4450 - val_loss: 0.4690\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4408 - val_loss: 0.4646\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4376 - val_loss: 0.4623\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4343 - val_loss: 0.4589\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4314 - val_loss: 0.4559\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4285 - val_loss: 0.4532\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4261 - val_loss: 0.4522\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4234 - val_loss: 0.4498\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4217 - val_loss: 0.4478\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4198 - val_loss: 0.4458\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4172 - val_loss: 0.4436\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4152 - val_loss: 0.4426\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4136 - val_loss: 0.4422\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4113 - val_loss: 0.4390\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4094 - val_loss: 0.4388\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4076 - val_loss: 0.4386\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4063 - val_loss: 0.4371\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4050 - val_loss: 0.4358\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4033 - val_loss: 0.4317\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4017 - val_loss: 0.4311\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3996 - val_loss: 0.4318\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3985 - val_loss: 0.4282\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3976 - val_loss: 0.4273\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3958 - val_loss: 0.4278\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3953 - val_loss: 0.4252\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3930 - val_loss: 0.4233\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3921 - val_loss: 0.4242\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3914 - val_loss: 0.4221\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3906 - val_loss: 0.4260\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3918 - val_loss: 0.4223\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3903 - val_loss: 0.4231\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3871 - val_loss: 0.4193\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3862 - val_loss: 0.4175\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3846 - val_loss: 0.4167\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3830 - val_loss: 0.4160\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3820 - val_loss: 0.4169\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3820 - val_loss: 0.4143\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3800 - val_loss: 0.4152\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3795 - val_loss: 0.4136\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3778 - val_loss: 0.4121\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3771 - val_loss: 0.4111\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3770 - val_loss: 0.4105\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3756 - val_loss: 0.4102\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3753 - val_loss: 0.4095\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3761 - val_loss: 0.4085\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3730 - val_loss: 0.4086\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3732 - val_loss: 0.4081\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3712 - val_loss: 0.4055\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3710 - val_loss: 0.4057\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3702 - val_loss: 0.4059\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3686 - val_loss: 0.4060\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3683 - val_loss: 0.4080\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3676 - val_loss: 0.4042\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3664 - val_loss: 0.4042\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3671 - val_loss: 0.4035\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3652 - val_loss: 0.4044\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3651 - val_loss: 0.4016\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3636 - val_loss: 0.4048\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3637 - val_loss: 0.4011\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3628 - val_loss: 0.4009\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3625 - val_loss: 0.3994\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3619 - val_loss: 0.4001\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3606 - val_loss: 0.4004\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3605 - val_loss: 0.3978\n",
            "Epoch 80/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3600 - val_loss: 0.4000\n",
            "Epoch 81/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3604 - val_loss: 0.4008\n",
            "Epoch 82/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3649 - val_loss: 0.3984\n",
            "Epoch 83/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3581 - val_loss: 0.3981\n",
            "Epoch 84/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3574 - val_loss: 0.3994\n",
            "Epoch 85/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3567 - val_loss: 0.3963\n",
            "Epoch 86/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3556 - val_loss: 0.3968\n",
            "Epoch 87/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3555 - val_loss: 0.3964\n",
            "Epoch 88/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3564 - val_loss: 0.3964\n",
            "Epoch 89/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3589 - val_loss: 0.3962\n",
            "Epoch 90/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3587 - val_loss: 0.3990\n",
            "Epoch 91/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3636 - val_loss: 0.3981\n",
            "Epoch 92/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3639 - val_loss: 0.3916\n",
            "Epoch 93/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3538 - val_loss: 0.3942\n",
            "Epoch 94/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3532 - val_loss: 0.3919\n",
            "Epoch 95/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3521 - val_loss: 0.3941\n",
            "Epoch 96/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3515 - val_loss: 0.3909\n",
            "Epoch 97/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3508 - val_loss: 0.3896\n",
            "Epoch 98/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3504 - val_loss: 0.3891\n",
            "Epoch 99/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3504 - val_loss: 0.3905\n",
            "Epoch 100/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3497 - val_loss: 0.3888\n",
            "121/121 [==============================] - 0s 2ms/step - loss: 0.3730\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "242/242 [==============================] - 1s 3ms/step - loss: 1.0598 - val_loss: 0.6931\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.5816 - val_loss: 0.5704\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.5174 - val_loss: 0.5175\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4908 - val_loss: 0.4923\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4651 - val_loss: 0.4784\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4508 - val_loss: 0.4650\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4384 - val_loss: 0.4632\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4300 - val_loss: 0.4554\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4205 - val_loss: 0.4501\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4134 - val_loss: 0.4508\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4070 - val_loss: 0.4464\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4011 - val_loss: 0.4523\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3958 - val_loss: 0.4521\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3922 - val_loss: 0.4468\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3860 - val_loss: 0.4457\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3835 - val_loss: 0.4422\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3836 - val_loss: 0.4475\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3740 - val_loss: 0.4449\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3688 - val_loss: 0.4594\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3667 - val_loss: 0.4519\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3638 - val_loss: 0.4454\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3597 - val_loss: 0.4530\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3575 - val_loss: 0.4396\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3530 - val_loss: 0.4454\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3506 - val_loss: 0.4478\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3525 - val_loss: 0.4447\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3469 - val_loss: 0.4349\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3430 - val_loss: 0.4416\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3418 - val_loss: 0.4383\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3387 - val_loss: 0.4393\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3362 - val_loss: 0.4324\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3348 - val_loss: 0.4341\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3326 - val_loss: 0.4334\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3325 - val_loss: 0.4330\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3306 - val_loss: 0.4560\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3288 - val_loss: 0.4339\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3260 - val_loss: 0.4325\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3256 - val_loss: 0.4260\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3223 - val_loss: 0.4365\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3218 - val_loss: 0.4281\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3220 - val_loss: 0.4597\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3233 - val_loss: 0.4223\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3173 - val_loss: 0.4437\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3154 - val_loss: 0.4159\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3157 - val_loss: 0.4196\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3122 - val_loss: 0.4196\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3125 - val_loss: 0.4107\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3110 - val_loss: 0.4279\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3117 - val_loss: 0.4120\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3083 - val_loss: 0.4247\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3085 - val_loss: 0.4062\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3062 - val_loss: 0.4098\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3044 - val_loss: 0.4099\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3040 - val_loss: 0.4048\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3021 - val_loss: 0.3934\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3021 - val_loss: 0.4074\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3014 - val_loss: 0.3924\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2999 - val_loss: 0.3923\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3005 - val_loss: 0.3926\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2971 - val_loss: 0.3948\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2967 - val_loss: 0.3984\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2994 - val_loss: 0.3803\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2968 - val_loss: 0.3870\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2948 - val_loss: 0.3805\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2956 - val_loss: 0.3772\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2926 - val_loss: 0.3770\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2906 - val_loss: 0.3819\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2911 - val_loss: 0.3958\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2933 - val_loss: 0.3709\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2892 - val_loss: 0.3902\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.2902 - val_loss: 0.3689\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2863 - val_loss: 0.3594\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2874 - val_loss: 0.3609\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2851 - val_loss: 0.3868\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2875 - val_loss: 0.3493\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2859 - val_loss: 0.3596\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.2847 - val_loss: 0.3502\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2848 - val_loss: 0.3484\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2851 - val_loss: 0.3458\n",
            "Epoch 80/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2820 - val_loss: 0.3483\n",
            "Epoch 81/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2830 - val_loss: 0.3473\n",
            "Epoch 82/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2803 - val_loss: 0.3477\n",
            "Epoch 83/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2819 - val_loss: 0.3466\n",
            "Epoch 84/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2799 - val_loss: 0.3449\n",
            "Epoch 85/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2813 - val_loss: 0.3408\n",
            "Epoch 86/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2821 - val_loss: 0.3376\n",
            "Epoch 87/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2776 - val_loss: 0.3334\n",
            "Epoch 88/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2788 - val_loss: 0.3603\n",
            "Epoch 89/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2758 - val_loss: 0.3469\n",
            "Epoch 90/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2765 - val_loss: 0.3457\n",
            "Epoch 91/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2789 - val_loss: 0.3281\n",
            "Epoch 92/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2726 - val_loss: 0.3296\n",
            "Epoch 93/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2743 - val_loss: 0.3402\n",
            "Epoch 94/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2738 - val_loss: 0.3313\n",
            "Epoch 95/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2724 - val_loss: 0.3310\n",
            "Epoch 96/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2736 - val_loss: 0.3326\n",
            "Epoch 97/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.2717 - val_loss: 0.3326\n",
            "Epoch 98/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2723 - val_loss: 0.3297\n",
            "Epoch 99/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2731 - val_loss: 0.3212\n",
            "Epoch 100/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2696 - val_loss: 0.3285\n",
            "121/121 [==============================] - 0s 2ms/step - loss: 0.3447\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "242/242 [==============================] - 1s 4ms/step - loss: 1.3509 - val_loss: 0.6752\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.6036 - val_loss: 0.5662\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.5176 - val_loss: 0.5094\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4786 - val_loss: 0.4859\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4497 - val_loss: 0.4632\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4315 - val_loss: 0.4495\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4179 - val_loss: 0.4371\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4091 - val_loss: 0.4361\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4012 - val_loss: 0.4250\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3946 - val_loss: 0.4168\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3895 - val_loss: 0.4185\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3841 - val_loss: 0.4120\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3783 - val_loss: 0.4093\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3759 - val_loss: 0.4049\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3728 - val_loss: 0.4062\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3696 - val_loss: 0.4035\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3653 - val_loss: 0.4005\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3619 - val_loss: 0.3914\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3574 - val_loss: 0.3881\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3531 - val_loss: 0.3880\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3541 - val_loss: 0.3825\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3460 - val_loss: 0.3846\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3461 - val_loss: 0.3804\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3437 - val_loss: 0.3760\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3400 - val_loss: 0.3723\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3374 - val_loss: 0.3691\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3331 - val_loss: 0.3764\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3305 - val_loss: 0.3676\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3286 - val_loss: 0.3698\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3293 - val_loss: 0.3653\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3306 - val_loss: 0.3702\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3273 - val_loss: 0.3679\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3244 - val_loss: 0.3558\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3204 - val_loss: 0.3574\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3179 - val_loss: 0.3544\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3176 - val_loss: 0.3541\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3127 - val_loss: 0.3506\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3107 - val_loss: 0.3563\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3123 - val_loss: 0.3559\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3077 - val_loss: 0.3489\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3085 - val_loss: 0.3431\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3045 - val_loss: 0.3507\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3032 - val_loss: 0.3448\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3022 - val_loss: 0.3495\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3033 - val_loss: 0.3469\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.2993 - val_loss: 0.3367\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2968 - val_loss: 0.3426\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2955 - val_loss: 0.3397\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2941 - val_loss: 0.3344\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2932 - val_loss: 0.3429\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2925 - val_loss: 0.3403\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2901 - val_loss: 0.3376\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2885 - val_loss: 0.3280\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2870 - val_loss: 0.3384\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2877 - val_loss: 0.3350\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2862 - val_loss: 0.3386\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.2842 - val_loss: 0.3379\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.2865 - val_loss: 0.3312\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2809 - val_loss: 0.3387\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2808 - val_loss: 0.3542\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2799 - val_loss: 0.3350\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 1s 5ms/step - loss: 0.2813 - val_loss: 0.3411\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 1s 5ms/step - loss: 0.2785 - val_loss: 0.3284\n",
            "121/121 [==============================] - 0s 2ms/step - loss: 0.3119\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "242/242 [==============================] - 2s 3ms/step - loss: 1.2336 - val_loss: 0.6019\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5523 - val_loss: 0.5366\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4916 - val_loss: 0.4928\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4603 - val_loss: 0.4683\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4397 - val_loss: 0.4471\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4118 - val_loss: 0.4397\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4003 - val_loss: 0.4288\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3879 - val_loss: 0.4152\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3763 - val_loss: 0.4110\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3680 - val_loss: 0.4073\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3637 - val_loss: 0.4189\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3643 - val_loss: 0.3918\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3533 - val_loss: 0.3866\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3488 - val_loss: 0.3822\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3432 - val_loss: 0.3819\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3456 - val_loss: 0.3926\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3369 - val_loss: 0.3756\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3364 - val_loss: 0.3729\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3319 - val_loss: 0.3709\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3297 - val_loss: 0.3690\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3290 - val_loss: 0.3806\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3356 - val_loss: 0.3766\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3322 - val_loss: 0.3782\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3244 - val_loss: 0.3597\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3206 - val_loss: 0.3624\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3169 - val_loss: 0.3621\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3124 - val_loss: 0.3570\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3127 - val_loss: 0.3521\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3117 - val_loss: 0.3529\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3088 - val_loss: 0.3487\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3087 - val_loss: 0.3499\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3064 - val_loss: 0.3458\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3039 - val_loss: 0.3500\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3022 - val_loss: 0.3559\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3020 - val_loss: 0.3531\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2994 - val_loss: 0.3459\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2972 - val_loss: 0.3413\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2950 - val_loss: 0.3456\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2973 - val_loss: 0.3417\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2925 - val_loss: 0.3398\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2926 - val_loss: 0.3382\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2917 - val_loss: 0.3387\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2894 - val_loss: 0.3405\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2880 - val_loss: 0.3356\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2866 - val_loss: 0.3502\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2842 - val_loss: 0.3374\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2857 - val_loss: 0.3387\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2844 - val_loss: 0.3384\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2815 - val_loss: 0.3354\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2814 - val_loss: 0.3333\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2816 - val_loss: 0.3338\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2793 - val_loss: 0.3324\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2799 - val_loss: 0.3323\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2773 - val_loss: 0.3425\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2753 - val_loss: 0.3422\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2768 - val_loss: 0.3331\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2745 - val_loss: 0.3327\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2753 - val_loss: 0.3368\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2745 - val_loss: 0.3280\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2738 - val_loss: 0.3392\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2726 - val_loss: 0.3335\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2752 - val_loss: 0.3330\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2721 - val_loss: 0.3285\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2716 - val_loss: 0.3256\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2684 - val_loss: 0.3315\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2681 - val_loss: 0.3364\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2680 - val_loss: 0.3319\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2665 - val_loss: 0.3258\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2685 - val_loss: 0.3267\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2663 - val_loss: 0.3285\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2643 - val_loss: 0.3292\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2641 - val_loss: 0.3323\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2643 - val_loss: 0.3216\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2634 - val_loss: 0.3281\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2653 - val_loss: 0.3237\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2633 - val_loss: 0.3454\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2607 - val_loss: 0.3313\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2623 - val_loss: 0.3347\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2626 - val_loss: 0.3254\n",
            "Epoch 80/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2616 - val_loss: 0.3254\n",
            "Epoch 81/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2610 - val_loss: 0.3219\n",
            "Epoch 82/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2615 - val_loss: 0.3452\n",
            "Epoch 83/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.2603 - val_loss: 0.3313\n",
            "121/121 [==============================] - 0s 2ms/step - loss: 0.3254\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "242/242 [==============================] - 1s 3ms/step - loss: 2.1545 - val_loss: 0.9768\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.8110 - val_loss: 0.8355\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7351 - val_loss: 0.7736\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6928 - val_loss: 0.7264\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6595 - val_loss: 0.6890\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6336 - val_loss: 0.6645\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6122 - val_loss: 0.6292\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5949 - val_loss: 0.6199\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5796 - val_loss: 0.5933\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5663 - val_loss: 0.5833\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5543 - val_loss: 0.5629\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5465 - val_loss: 0.5515\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5371 - val_loss: 0.5416\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5311 - val_loss: 0.5360\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5228 - val_loss: 0.5265\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5181 - val_loss: 0.5219\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5114 - val_loss: 0.5455\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5063 - val_loss: 0.5132\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5039 - val_loss: 0.5116\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4973 - val_loss: 0.5055\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4947 - val_loss: 0.5076\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4903 - val_loss: 0.5071\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4874 - val_loss: 0.5010\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4834 - val_loss: 0.5105\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4781 - val_loss: 0.4986\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4784 - val_loss: 0.5010\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4739 - val_loss: 0.4966\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4732 - val_loss: 0.5083\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4665 - val_loss: 0.4997\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4667 - val_loss: 0.5329\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4630 - val_loss: 0.4990\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4620 - val_loss: 0.5123\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4583 - val_loss: 0.5037\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4569 - val_loss: 0.5061\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4544 - val_loss: 0.5078\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4532 - val_loss: 0.5240\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4495 - val_loss: 0.5122\n",
            "121/121 [==============================] - 0s 2ms/step - loss: 0.7040\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "242/242 [==============================] - 1s 3ms/step - loss: 1.9453 - val_loss: 0.8922\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.7914 - val_loss: 0.7452\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.7064 - val_loss: 0.6998\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.6662 - val_loss: 0.6635\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6317 - val_loss: 0.6339\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6052 - val_loss: 0.6110\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5839 - val_loss: 0.5897\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5634 - val_loss: 0.5732\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.5480 - val_loss: 0.5584\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5338 - val_loss: 0.5468\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5236 - val_loss: 0.5355\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5123 - val_loss: 0.5279\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5055 - val_loss: 0.5191\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4962 - val_loss: 0.5138\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4914 - val_loss: 0.5062\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4844 - val_loss: 0.5032\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4809 - val_loss: 0.4972\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4740 - val_loss: 0.4937\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4704 - val_loss: 0.4913\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4682 - val_loss: 0.4846\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4631 - val_loss: 0.4813\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4591 - val_loss: 0.4785\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4556 - val_loss: 0.4748\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4526 - val_loss: 0.4726\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4499 - val_loss: 0.4706\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4467 - val_loss: 0.4684\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4447 - val_loss: 0.4673\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4445 - val_loss: 0.4663\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4434 - val_loss: 0.4704\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4450 - val_loss: 0.4623\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4446 - val_loss: 0.4582\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4348 - val_loss: 0.4566\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4324 - val_loss: 0.4564\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4373 - val_loss: 0.4548\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4294 - val_loss: 0.4548\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4301 - val_loss: 0.4577\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4318 - val_loss: 0.4516\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4338 - val_loss: 0.4504\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4252 - val_loss: 0.4487\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4304 - val_loss: 0.4473\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4227 - val_loss: 0.4464\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4271 - val_loss: 0.4453\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4199 - val_loss: 0.4441\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4200 - val_loss: 0.4471\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4253 - val_loss: 0.4459\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4352 - val_loss: 0.4427\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4162 - val_loss: 0.4397\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4134 - val_loss: 0.4387\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4131 - val_loss: 0.4377\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4117 - val_loss: 0.4372\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4111 - val_loss: 0.4384\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4111 - val_loss: 0.4349\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4093 - val_loss: 0.4366\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4090 - val_loss: 0.4330\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4068 - val_loss: 0.4324\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4063 - val_loss: 0.4318\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4055 - val_loss: 0.4332\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4054 - val_loss: 0.4308\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4029 - val_loss: 0.4296\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4028 - val_loss: 0.4308\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4054 - val_loss: 0.4309\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4045 - val_loss: 0.4372\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4084 - val_loss: 0.4297\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4149 - val_loss: 0.4286\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4006 - val_loss: 0.4254\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3979 - val_loss: 0.4271\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4050 - val_loss: 0.4279\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4141 - val_loss: 0.4262\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4013 - val_loss: 0.4256\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4052 - val_loss: 0.4232\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3948 - val_loss: 0.4211\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3933 - val_loss: 0.4204\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3930 - val_loss: 0.4212\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3927 - val_loss: 0.4198\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3913 - val_loss: 0.4203\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3910 - val_loss: 0.4184\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3902 - val_loss: 0.4188\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3939 - val_loss: 0.4174\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3886 - val_loss: 0.4173\n",
            "Epoch 80/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3881 - val_loss: 0.4177\n",
            "Epoch 81/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3902 - val_loss: 0.4159\n",
            "Epoch 82/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3867 - val_loss: 0.4154\n",
            "Epoch 83/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3866 - val_loss: 0.4165\n",
            "Epoch 84/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3866 - val_loss: 0.4149\n",
            "Epoch 85/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3850 - val_loss: 0.4146\n",
            "Epoch 86/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3840 - val_loss: 0.4129\n",
            "Epoch 87/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3840 - val_loss: 0.4137\n",
            "Epoch 88/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3831 - val_loss: 0.4127\n",
            "Epoch 89/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3841 - val_loss: 0.4117\n",
            "Epoch 90/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3820 - val_loss: 0.4115\n",
            "Epoch 91/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3818 - val_loss: 0.4107\n",
            "Epoch 92/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3808 - val_loss: 0.4118\n",
            "Epoch 93/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3808 - val_loss: 0.4102\n",
            "Epoch 94/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3807 - val_loss: 0.4110\n",
            "Epoch 95/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3842 - val_loss: 0.4119\n",
            "Epoch 96/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3902 - val_loss: 0.4095\n",
            "Epoch 97/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3818 - val_loss: 0.4104\n",
            "Epoch 98/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3809 - val_loss: 0.4138\n",
            "Epoch 99/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3941 - val_loss: 0.4136\n",
            "Epoch 100/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3865 - val_loss: 0.4150\n",
            "121/121 [==============================] - 0s 2ms/step - loss: 0.3933\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "242/242 [==============================] - 1s 3ms/step - loss: 2.2486 - val_loss: 0.8941\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7522 - val_loss: 0.6730\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6520 - val_loss: 0.6349\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6148 - val_loss: 0.6072\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5890 - val_loss: 0.5870\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5661 - val_loss: 0.5687\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5487 - val_loss: 0.5537\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5326 - val_loss: 0.5424\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5204 - val_loss: 0.5324\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5121 - val_loss: 0.5252\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5021 - val_loss: 0.5183\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4957 - val_loss: 0.5139\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4901 - val_loss: 0.5066\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4844 - val_loss: 0.5031\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4803 - val_loss: 0.4989\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4751 - val_loss: 0.4958\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4712 - val_loss: 0.4924\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4691 - val_loss: 0.4895\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4650 - val_loss: 0.4883\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4621 - val_loss: 0.4857\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4608 - val_loss: 0.4822\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4573 - val_loss: 0.4805\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4549 - val_loss: 0.4790\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4529 - val_loss: 0.4773\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4511 - val_loss: 0.4753\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4478 - val_loss: 0.4737\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4470 - val_loss: 0.4748\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4453 - val_loss: 0.4692\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4429 - val_loss: 0.4676\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4414 - val_loss: 0.4659\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4395 - val_loss: 0.4644\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4380 - val_loss: 0.4635\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4366 - val_loss: 0.4627\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4346 - val_loss: 0.4627\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4335 - val_loss: 0.4616\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4321 - val_loss: 0.4593\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4303 - val_loss: 0.4596\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4290 - val_loss: 0.4559\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4278 - val_loss: 0.4555\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4266 - val_loss: 0.4529\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4251 - val_loss: 0.4528\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4240 - val_loss: 0.4514\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4230 - val_loss: 0.4507\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4219 - val_loss: 0.4480\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4201 - val_loss: 0.4479\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4192 - val_loss: 0.4467\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4182 - val_loss: 0.4451\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4167 - val_loss: 0.4452\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4153 - val_loss: 0.4435\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4151 - val_loss: 0.4431\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4135 - val_loss: 0.4411\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4128 - val_loss: 0.4409\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4119 - val_loss: 0.4395\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4106 - val_loss: 0.4392\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4097 - val_loss: 0.4383\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4084 - val_loss: 0.4370\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4082 - val_loss: 0.4389\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4077 - val_loss: 0.4350\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4057 - val_loss: 0.4349\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4045 - val_loss: 0.4341\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4044 - val_loss: 0.4336\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4035 - val_loss: 0.4328\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4018 - val_loss: 0.4325\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4014 - val_loss: 0.4311\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4010 - val_loss: 0.4299\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3998 - val_loss: 0.4299\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3985 - val_loss: 0.4297\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3986 - val_loss: 0.4296\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3979 - val_loss: 0.4276\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3967 - val_loss: 0.4268\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3957 - val_loss: 0.4263\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3950 - val_loss: 0.4250\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3943 - val_loss: 0.4259\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3934 - val_loss: 0.4248\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3924 - val_loss: 0.4246\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3915 - val_loss: 0.4230\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3915 - val_loss: 0.4241\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3907 - val_loss: 0.4238\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3910 - val_loss: 0.4217\n",
            "Epoch 80/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3891 - val_loss: 0.4211\n",
            "Epoch 81/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3900 - val_loss: 0.4195\n",
            "Epoch 82/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3875 - val_loss: 0.4193\n",
            "Epoch 83/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3881 - val_loss: 0.4190\n",
            "Epoch 84/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3863 - val_loss: 0.4197\n",
            "Epoch 85/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3859 - val_loss: 0.4176\n",
            "Epoch 86/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3868 - val_loss: 0.4172\n",
            "Epoch 87/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3845 - val_loss: 0.4171\n",
            "Epoch 88/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3844 - val_loss: 0.4161\n",
            "Epoch 89/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3831 - val_loss: 0.4146\n",
            "Epoch 90/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3830 - val_loss: 0.4152\n",
            "Epoch 91/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3824 - val_loss: 0.4149\n",
            "Epoch 92/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3816 - val_loss: 0.4140\n",
            "Epoch 93/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3811 - val_loss: 0.4135\n",
            "Epoch 94/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3806 - val_loss: 0.4135\n",
            "Epoch 95/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3804 - val_loss: 0.4125\n",
            "Epoch 96/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3790 - val_loss: 0.4121\n",
            "Epoch 97/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3791 - val_loss: 0.4119\n",
            "Epoch 98/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3783 - val_loss: 0.4104\n",
            "Epoch 99/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3775 - val_loss: 0.4108\n",
            "Epoch 100/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3771 - val_loss: 0.4109\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 0.4034\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "242/242 [==============================] - 1s 2ms/step - loss: 2.7982 - val_loss: 1.0370\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7215 - val_loss: 0.6114\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5710 - val_loss: 0.5689\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5559 - val_loss: 0.5638\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5503 - val_loss: 0.5555\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5476 - val_loss: 0.5549\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5439 - val_loss: 0.5646\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5412 - val_loss: 0.5494\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5405 - val_loss: 0.5487\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5381 - val_loss: 0.5651\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5371 - val_loss: 0.5525\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5350 - val_loss: 0.5476\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5347 - val_loss: 0.5696\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5344 - val_loss: 0.5684\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5329 - val_loss: 0.5579\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5329 - val_loss: 0.5685\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5323 - val_loss: 0.5720\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5310 - val_loss: 0.5664\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5310 - val_loss: 0.5941\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5311 - val_loss: 0.5874\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5297 - val_loss: 0.5946\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5264 - val_loss: 0.5765\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 0.6972\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "242/242 [==============================] - 1s 2ms/step - loss: 3.5176 - val_loss: 1.3561\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.0197 - val_loss: 0.8249\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7661 - val_loss: 0.7583\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7118 - val_loss: 0.7170\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6796 - val_loss: 0.6926\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6553 - val_loss: 0.6696\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6351 - val_loss: 0.6473\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6182 - val_loss: 0.6306\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6057 - val_loss: 0.6217\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5919 - val_loss: 0.6024\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5845 - val_loss: 0.5940\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5760 - val_loss: 0.5845\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5713 - val_loss: 0.5814\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5643 - val_loss: 0.5742\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5610 - val_loss: 0.5788\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5547 - val_loss: 0.5834\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5513 - val_loss: 0.5620\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5513 - val_loss: 0.5636\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5452 - val_loss: 0.5539\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5495 - val_loss: 0.5662\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5441 - val_loss: 0.5576\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5409 - val_loss: 0.5478\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5440 - val_loss: 0.5494\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5408 - val_loss: 0.5598\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.5394 - val_loss: 0.5642\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5377 - val_loss: 0.5466\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5368 - val_loss: 0.5430\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5385 - val_loss: 0.5686\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5343 - val_loss: 0.5433\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5364 - val_loss: 0.5414\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5381 - val_loss: 0.5455\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5383 - val_loss: 0.5457\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5342 - val_loss: 0.5404\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5367 - val_loss: 0.5419\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5377 - val_loss: 0.5515\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5335 - val_loss: 0.5400\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5336 - val_loss: 0.5699\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5297 - val_loss: 0.5368\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5347 - val_loss: 0.5356\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5348 - val_loss: 0.5546\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5347 - val_loss: 0.5423\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5332 - val_loss: 0.5405\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5319 - val_loss: 0.5356\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5359 - val_loss: 0.5607\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5346 - val_loss: 0.5535\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5340 - val_loss: 0.5431\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5345 - val_loss: 0.5418\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5322 - val_loss: 0.5364\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5346 - val_loss: 0.5513\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5347 - val_loss: 0.5478\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5299 - val_loss: 0.5383\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5323 - val_loss: 0.5348\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5360 - val_loss: 0.5386\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5336 - val_loss: 0.5374\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5330 - val_loss: 0.5364\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5348 - val_loss: 0.5391\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5349 - val_loss: 0.5484\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5309 - val_loss: 0.5664\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5319 - val_loss: 0.5363\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5346 - val_loss: 0.5398\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5323 - val_loss: 0.5668\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5342 - val_loss: 0.5505\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 0.5251\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "242/242 [==============================] - 1s 2ms/step - loss: 2.7878 - val_loss: 1.0600\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7955 - val_loss: 0.6842\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6348 - val_loss: 0.6405\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6060 - val_loss: 0.6207\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5900 - val_loss: 0.6066\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5774 - val_loss: 0.5949\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5675 - val_loss: 0.5851\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5585 - val_loss: 0.5773\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5514 - val_loss: 0.5688\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5452 - val_loss: 0.5633\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5405 - val_loss: 0.5604\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5363 - val_loss: 0.5561\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5323 - val_loss: 0.5503\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5303 - val_loss: 0.5479\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5275 - val_loss: 0.5467\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5252 - val_loss: 0.5426\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 0.5418\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5223 - val_loss: 0.5425\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5213 - val_loss: 0.5407\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5206 - val_loss: 0.5395\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5194 - val_loss: 0.5392\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5188 - val_loss: 0.5377\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5181 - val_loss: 0.5387\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5177 - val_loss: 0.5378\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5173 - val_loss: 0.5352\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5169 - val_loss: 0.5346\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5166 - val_loss: 0.5346\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5163 - val_loss: 0.5337\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5159 - val_loss: 0.5327\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5167 - val_loss: 0.5347\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5158 - val_loss: 0.5350\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5157 - val_loss: 0.5345\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5158 - val_loss: 0.5342\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5154 - val_loss: 0.5332\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5159 - val_loss: 0.5339\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5153 - val_loss: 0.5328\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5155 - val_loss: 0.5336\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5154 - val_loss: 0.5334\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5152 - val_loss: 0.5330\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 0.5613\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "242/242 [==============================] - 1s 2ms/step - loss: 1.7167 - val_loss: 0.9121\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6452 - val_loss: 0.8037\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6015 - val_loss: 0.7324\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5784 - val_loss: 0.6871\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5675 - val_loss: 0.6481\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5594 - val_loss: 0.6155\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5494 - val_loss: 0.5836\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5578 - val_loss: 0.5717\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5496 - val_loss: 0.5685\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5460 - val_loss: 0.5558\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5481 - val_loss: 0.5424\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5454 - val_loss: 0.5421\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5562 - val_loss: 0.5434\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5390 - val_loss: 0.5440\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5517 - val_loss: 0.5543\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5393 - val_loss: 0.5568\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5413 - val_loss: 0.5630\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5386 - val_loss: 0.5794\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5311 - val_loss: 0.7705\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5449 - val_loss: 0.6244\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5281 - val_loss: 0.7684\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5325 - val_loss: 0.6404\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 1.0504\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "242/242 [==============================] - 1s 2ms/step - loss: 1.7221 - val_loss: 0.9129\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6966 - val_loss: 0.6843\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6318 - val_loss: 0.6520\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.9868 - val_loss: 0.6091\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.8623 - val_loss: 0.5854\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5634 - val_loss: 0.5764\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5576 - val_loss: 0.5469\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5623 - val_loss: 0.5393\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5428 - val_loss: 0.5450\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5346 - val_loss: 0.5380\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5566 - val_loss: 0.5398\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5412 - val_loss: 0.5463\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5533 - val_loss: 0.5415\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5395 - val_loss: 0.5635\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6322 - val_loss: 0.5618\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5449 - val_loss: 0.6062\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6402 - val_loss: 0.5669\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5542 - val_loss: 0.5925\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5576 - val_loss: 0.5566\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5467 - val_loss: 0.7057\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 0.5714\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "242/242 [==============================] - 1s 2ms/step - loss: 1.7319 - val_loss: 0.7511\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6851 - val_loss: 0.6657\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6157 - val_loss: 0.6155\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5803 - val_loss: 0.5840\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5517 - val_loss: 0.5794\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5453 - val_loss: 0.5546\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5331 - val_loss: 0.5537\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5302 - val_loss: 0.5390\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 0.5503\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 0.5343\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5185 - val_loss: 0.5481\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5376 - val_loss: 0.5403\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5503 - val_loss: 0.5343\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5312 - val_loss: 0.5412\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5442 - val_loss: 0.5352\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 0.5402\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5169 - val_loss: 0.5533\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5276 - val_loss: 0.5410\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5184 - val_loss: 0.5406\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5576 - val_loss: 0.5394\n",
            "121/121 [==============================] - 0s 2ms/step - loss: 0.5694\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "242/242 [==============================] - 1s 3ms/step - loss: 1.1154 - val_loss: 0.8376\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6769 - val_loss: 0.7194\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6128 - val_loss: 0.6510\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5716 - val_loss: 0.6047\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5403 - val_loss: 0.5659\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5152 - val_loss: 0.5369\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4964 - val_loss: 0.5190\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4833 - val_loss: 0.5096\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4786 - val_loss: 0.4992\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4709 - val_loss: 0.4879\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4650 - val_loss: 0.4786\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4615 - val_loss: 0.4754\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4563 - val_loss: 0.4710\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4548 - val_loss: 0.4726\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4506 - val_loss: 0.4632\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4442 - val_loss: 0.4569\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4420 - val_loss: 0.4552\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4388 - val_loss: 0.4593\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4382 - val_loss: 0.4521\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4325 - val_loss: 0.4458\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4312 - val_loss: 0.4454\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4267 - val_loss: 0.4437\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4232 - val_loss: 0.4429\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4206 - val_loss: 0.4365\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4180 - val_loss: 0.4334\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4164 - val_loss: 0.4318\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4157 - val_loss: 0.4438\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4131 - val_loss: 0.4271\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4097 - val_loss: 0.4268\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4074 - val_loss: 0.4270\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4049 - val_loss: 0.4283\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4029 - val_loss: 0.4231\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4017 - val_loss: 0.4294\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3999 - val_loss: 0.4308\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3980 - val_loss: 0.4210\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3966 - val_loss: 0.4225\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3959 - val_loss: 0.4207\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3939 - val_loss: 0.4142\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3916 - val_loss: 0.4238\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3911 - val_loss: 0.4150\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3885 - val_loss: 0.4191\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3876 - val_loss: 0.4168\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3876 - val_loss: 0.4092\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3865 - val_loss: 0.4114\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3848 - val_loss: 0.4097\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3837 - val_loss: 0.4056\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3846 - val_loss: 0.4041\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3833 - val_loss: 0.4055\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3826 - val_loss: 0.4048\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3816 - val_loss: 0.4081\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3813 - val_loss: 0.4079\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3793 - val_loss: 0.4011\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3793 - val_loss: 0.3984\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3790 - val_loss: 0.4018\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3775 - val_loss: 0.4052\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3784 - val_loss: 0.3992\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3734 - val_loss: 0.4008\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3746 - val_loss: 0.4008\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3751 - val_loss: 0.3950\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3726 - val_loss: 0.4009\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3718 - val_loss: 0.3935\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3717 - val_loss: 0.3951\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3711 - val_loss: 0.3906\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3711 - val_loss: 0.3912\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3674 - val_loss: 0.3919\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3679 - val_loss: 0.3879\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3669 - val_loss: 0.3894\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3668 - val_loss: 0.3970\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3659 - val_loss: 0.3843\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3650 - val_loss: 0.3928\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3638 - val_loss: 0.3914\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3639 - val_loss: 0.3955\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3603 - val_loss: 0.3973\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3608 - val_loss: 0.3860\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3602 - val_loss: 0.3910\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3578 - val_loss: 0.3919\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3589 - val_loss: 0.4053\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3572 - val_loss: 0.3861\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3569 - val_loss: 0.3854\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 0.4145\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "242/242 [==============================] - 1s 3ms/step - loss: 1.1696 - val_loss: 0.7238\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6572 - val_loss: 0.5979\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5503 - val_loss: 0.5431\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5156 - val_loss: 0.5187\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5044 - val_loss: 0.5258\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4955 - val_loss: 0.4974\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4817 - val_loss: 0.4965\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4725 - val_loss: 0.4785\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4776 - val_loss: 0.4827\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4607 - val_loss: 0.4697\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4697 - val_loss: 0.4843\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4531 - val_loss: 0.4667\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4556 - val_loss: 0.4655\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4467 - val_loss: 0.4636\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4434 - val_loss: 0.4745\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4419 - val_loss: 0.4677\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4407 - val_loss: 0.4758\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4393 - val_loss: 0.4563\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4291 - val_loss: 0.4426\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4264 - val_loss: 0.4401\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4409 - val_loss: 0.4828\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4380 - val_loss: 0.4578\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4228 - val_loss: 0.4351\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4278 - val_loss: 0.4463\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4181 - val_loss: 0.4348\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4186 - val_loss: 0.4328\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4112 - val_loss: 0.4413\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4091 - val_loss: 0.4239\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4162 - val_loss: 0.5353\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4422 - val_loss: 0.4416\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4132 - val_loss: 0.4228\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4053 - val_loss: 0.4292\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4027 - val_loss: 0.4344\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4019 - val_loss: 0.4135\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3975 - val_loss: 0.4133\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4015 - val_loss: 0.4168\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3942 - val_loss: 0.4092\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4030 - val_loss: 0.4105\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3939 - val_loss: 0.4061\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3883 - val_loss: 0.4215\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3900 - val_loss: 0.4107\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4016 - val_loss: 0.4153\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3879 - val_loss: 0.4011\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3833 - val_loss: 0.4038\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3836 - val_loss: 0.4024\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3994 - val_loss: 0.4044\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3816 - val_loss: 0.3993\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3810 - val_loss: 0.3999\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3784 - val_loss: 0.3932\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3795 - val_loss: 0.3987\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3788 - val_loss: 0.3956\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3996 - val_loss: 0.4261\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3931 - val_loss: 0.3960\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3782 - val_loss: 0.3973\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3747 - val_loss: 0.3911\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3736 - val_loss: 0.3901\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3716 - val_loss: 0.4078\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3749 - val_loss: 0.3900\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3724 - val_loss: 0.3905\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3718 - val_loss: 0.3955\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3683 - val_loss: 0.3944\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3677 - val_loss: 0.3982\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3690 - val_loss: 0.3902\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3681 - val_loss: 0.3957\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3667 - val_loss: 0.3869\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3670 - val_loss: 0.3842\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3646 - val_loss: 0.3822\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3629 - val_loss: 0.3829\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3638 - val_loss: 0.3825\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3626 - val_loss: 0.3859\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3612 - val_loss: 0.3784\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3613 - val_loss: 0.3789\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3592 - val_loss: 0.3794\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3600 - val_loss: 0.3859\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3569 - val_loss: 0.3794\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3566 - val_loss: 0.3778\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3562 - val_loss: 0.3752\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3537 - val_loss: 0.3947\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3548 - val_loss: 0.3769\n",
            "Epoch 80/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3569 - val_loss: 0.3757\n",
            "Epoch 81/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3538 - val_loss: 0.3736\n",
            "Epoch 82/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3522 - val_loss: 0.3783\n",
            "Epoch 83/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3593 - val_loss: 0.3756\n",
            "Epoch 84/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3527 - val_loss: 0.3739\n",
            "Epoch 85/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3512 - val_loss: 0.3949\n",
            "Epoch 86/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3530 - val_loss: 0.3772\n",
            "Epoch 87/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3494 - val_loss: 0.3736\n",
            "Epoch 88/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3484 - val_loss: 0.3704\n",
            "Epoch 89/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3492 - val_loss: 0.3739\n",
            "Epoch 90/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3453 - val_loss: 0.3799\n",
            "Epoch 91/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3469 - val_loss: 0.3704\n",
            "Epoch 92/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3452 - val_loss: 0.3768\n",
            "Epoch 93/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3449 - val_loss: 0.3700\n",
            "Epoch 94/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3456 - val_loss: 0.3648\n",
            "Epoch 95/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3432 - val_loss: 0.3663\n",
            "Epoch 96/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3440 - val_loss: 0.3700\n",
            "Epoch 97/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3417 - val_loss: 0.3673\n",
            "Epoch 98/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3419 - val_loss: 0.3631\n",
            "Epoch 99/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3401 - val_loss: 0.3680\n",
            "Epoch 100/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3490 - val_loss: 0.3729\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 0.3660\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "242/242 [==============================] - 1s 3ms/step - loss: 1.4637 - val_loss: 0.5868\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5407 - val_loss: 0.5328\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5033 - val_loss: 0.5172\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4841 - val_loss: 0.4999\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4756 - val_loss: 0.4839\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4675 - val_loss: 0.4797\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4596 - val_loss: 0.4721\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4540 - val_loss: 0.4679\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4487 - val_loss: 0.4646\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4433 - val_loss: 0.4604\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4397 - val_loss: 0.4555\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4347 - val_loss: 0.4525\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4316 - val_loss: 0.4476\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4278 - val_loss: 0.4455\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4246 - val_loss: 0.4400\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4204 - val_loss: 0.4426\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4175 - val_loss: 0.4354\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4136 - val_loss: 0.4304\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4105 - val_loss: 0.4263\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4078 - val_loss: 0.4238\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4048 - val_loss: 0.4210\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4019 - val_loss: 0.4220\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3986 - val_loss: 0.4233\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3957 - val_loss: 0.4171\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3942 - val_loss: 0.4161\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3918 - val_loss: 0.4114\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3892 - val_loss: 0.4079\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3877 - val_loss: 0.4098\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3842 - val_loss: 0.4092\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3832 - val_loss: 0.4052\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3810 - val_loss: 0.4040\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3793 - val_loss: 0.4002\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3774 - val_loss: 0.4014\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3756 - val_loss: 0.4011\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3742 - val_loss: 0.3958\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3736 - val_loss: 0.3975\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3710 - val_loss: 0.3970\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3693 - val_loss: 0.4035\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3685 - val_loss: 0.3967\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3664 - val_loss: 0.3972\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3645 - val_loss: 0.3929\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3624 - val_loss: 0.3875\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3617 - val_loss: 0.3887\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3601 - val_loss: 0.3890\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3601 - val_loss: 0.3906\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3584 - val_loss: 0.3887\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3564 - val_loss: 0.3858\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3553 - val_loss: 0.3833\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3548 - val_loss: 0.3816\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3536 - val_loss: 0.3827\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3530 - val_loss: 0.3812\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3512 - val_loss: 0.3811\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3507 - val_loss: 0.3814\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3513 - val_loss: 0.3773\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3496 - val_loss: 0.3787\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3495 - val_loss: 0.3844\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3478 - val_loss: 0.3779\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3469 - val_loss: 0.3764\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3464 - val_loss: 0.3824\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3457 - val_loss: 0.3818\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3443 - val_loss: 0.3744\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3448 - val_loss: 0.3807\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3434 - val_loss: 0.3745\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3425 - val_loss: 0.3814\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3414 - val_loss: 0.3737\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3405 - val_loss: 0.3901\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3410 - val_loss: 0.3705\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3391 - val_loss: 0.3680\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3376 - val_loss: 0.3703\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3387 - val_loss: 0.3786\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3371 - val_loss: 0.3696\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3342 - val_loss: 0.3713\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3363 - val_loss: 0.3658\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3348 - val_loss: 0.3661\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3328 - val_loss: 0.3687\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3340 - val_loss: 0.3693\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3336 - val_loss: 0.3684\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3317 - val_loss: 0.3637\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3323 - val_loss: 0.3727\n",
            "Epoch 80/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3310 - val_loss: 0.3670\n",
            "Epoch 81/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3301 - val_loss: 0.3609\n",
            "Epoch 82/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3292 - val_loss: 0.3715\n",
            "Epoch 83/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3288 - val_loss: 0.3661\n",
            "Epoch 84/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3287 - val_loss: 0.3604\n",
            "Epoch 85/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3266 - val_loss: 0.3602\n",
            "Epoch 86/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3261 - val_loss: 0.3633\n",
            "Epoch 87/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3250 - val_loss: 0.3668\n",
            "Epoch 88/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3243 - val_loss: 0.3598\n",
            "Epoch 89/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3241 - val_loss: 0.3571\n",
            "Epoch 90/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3249 - val_loss: 0.3615\n",
            "Epoch 91/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3234 - val_loss: 0.3672\n",
            "Epoch 92/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3231 - val_loss: 0.3560\n",
            "Epoch 93/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3229 - val_loss: 0.3551\n",
            "Epoch 94/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3214 - val_loss: 0.3540\n",
            "Epoch 95/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3212 - val_loss: 0.3532\n",
            "Epoch 96/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3198 - val_loss: 0.3588\n",
            "Epoch 97/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3191 - val_loss: 0.3570\n",
            "Epoch 98/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3193 - val_loss: 0.3554\n",
            "Epoch 99/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3173 - val_loss: 0.3535\n",
            "Epoch 100/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3188 - val_loss: 0.3525\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 0.3444\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "242/242 [==============================] - 1s 2ms/step - loss: 2.0001 - val_loss: 0.8308\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6473 - val_loss: 0.7596\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6007 - val_loss: 0.7523\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5757 - val_loss: 0.7623\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5575 - val_loss: 0.7517\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5475 - val_loss: 0.7631\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5411 - val_loss: 0.7850\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5335 - val_loss: 0.7850\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5323 - val_loss: 0.8057\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5286 - val_loss: 0.8194\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5287 - val_loss: 0.8396\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5258 - val_loss: 0.9013\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5179 - val_loss: 0.8620\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5226 - val_loss: 0.9422\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 0.9993\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 2.4666\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "242/242 [==============================] - 1s 2ms/step - loss: 2.5900 - val_loss: 0.8354\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.9008 - val_loss: 0.7355\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.8353 - val_loss: 0.6569\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7469 - val_loss: 0.6368\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6720 - val_loss: 0.5897\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5837 - val_loss: 0.5811\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5629 - val_loss: 0.5745\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5676 - val_loss: 0.5651\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5649 - val_loss: 0.5771\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5410 - val_loss: 0.5490\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5483 - val_loss: 0.5646\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5525 - val_loss: 0.5956\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5509 - val_loss: 0.5398\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5563 - val_loss: 0.5498\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5570 - val_loss: 0.5386\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5398 - val_loss: 0.6601\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5449 - val_loss: 0.6263\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5462 - val_loss: 0.5415\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5422 - val_loss: 0.5624\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5375 - val_loss: 0.5396\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5342 - val_loss: 0.5346\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5344 - val_loss: 0.5329\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5478 - val_loss: 0.5754\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5402 - val_loss: 0.5656\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5449 - val_loss: 0.5529\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5398 - val_loss: 0.5521\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5380 - val_loss: 0.5363\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5381 - val_loss: 0.5349\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5422 - val_loss: 0.5380\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5412 - val_loss: 0.5423\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5439 - val_loss: 0.5364\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5421 - val_loss: 0.5368\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 0.5227\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "242/242 [==============================] - 1s 2ms/step - loss: 1.7742 - val_loss: 0.6350\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5817 - val_loss: 0.5714\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5954 - val_loss: 0.5548\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5741 - val_loss: 0.5635\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5876 - val_loss: 0.5463\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5340 - val_loss: 0.5460\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5280 - val_loss: 0.5539\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5778 - val_loss: 0.5535\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5283 - val_loss: 0.5468\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5290 - val_loss: 0.5421\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5193 - val_loss: 0.5468\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5295 - val_loss: 0.5355\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5185 - val_loss: 0.5337\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5182 - val_loss: 0.5414\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5159 - val_loss: 0.5309\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5167 - val_loss: 0.5324\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5160 - val_loss: 0.5311\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5168 - val_loss: 0.5307\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5164 - val_loss: 0.5313\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5172 - val_loss: 0.5343\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5231 - val_loss: 0.5350\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5286 - val_loss: 0.5336\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5170 - val_loss: 0.5317\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5155 - val_loss: 0.5321\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5172 - val_loss: 0.5351\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5179 - val_loss: 0.5333\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5162 - val_loss: 0.5307\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5160 - val_loss: 0.5304\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5174 - val_loss: 0.5373\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5159 - val_loss: 0.5326\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5164 - val_loss: 0.5322\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5167 - val_loss: 0.5317\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5149 - val_loss: 0.5405\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5194 - val_loss: 0.5500\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5183 - val_loss: 0.5347\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5263 - val_loss: 0.5456\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5348 - val_loss: 0.5396\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5192 - val_loss: 0.5351\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 0.5565\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "242/242 [==============================] - 1s 3ms/step - loss: 2.0463 - val_loss: 0.8600\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7119 - val_loss: 0.6866\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6423 - val_loss: 0.6398\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6077 - val_loss: 0.6084\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5822 - val_loss: 0.5822\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5612 - val_loss: 0.5659\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5434 - val_loss: 0.5462\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5269 - val_loss: 0.5345\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5129 - val_loss: 0.5211\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4996 - val_loss: 0.5092\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4880 - val_loss: 0.5100\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4773 - val_loss: 0.4939\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4669 - val_loss: 0.4887\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4573 - val_loss: 0.4826\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4495 - val_loss: 0.4802\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4428 - val_loss: 0.4810\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4368 - val_loss: 0.4778\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4317 - val_loss: 0.4784\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4276 - val_loss: 0.4801\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4239 - val_loss: 0.4786\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4203 - val_loss: 0.4831\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4175 - val_loss: 0.4830\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4144 - val_loss: 0.4865\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4120 - val_loss: 0.4856\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4095 - val_loss: 0.4881\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4073 - val_loss: 0.4893\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4055 - val_loss: 0.4900\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 0.7396\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "242/242 [==============================] - 1s 3ms/step - loss: 1.7871 - val_loss: 0.7569\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7123 - val_loss: 0.6915\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6502 - val_loss: 0.6499\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6108 - val_loss: 0.6138\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5784 - val_loss: 0.5864\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5513 - val_loss: 0.5620\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5291 - val_loss: 0.5449\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5111 - val_loss: 0.5304\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4950 - val_loss: 0.5142\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4830 - val_loss: 0.5021\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4709 - val_loss: 0.4911\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4619 - val_loss: 0.4815\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4541 - val_loss: 0.4785\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4475 - val_loss: 0.4702\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4402 - val_loss: 0.4625\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4353 - val_loss: 0.4636\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4303 - val_loss: 0.4531\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4275 - val_loss: 0.4530\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4226 - val_loss: 0.4449\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4191 - val_loss: 0.4420\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4149 - val_loss: 0.4391\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4125 - val_loss: 0.4357\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4090 - val_loss: 0.4349\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4070 - val_loss: 0.4346\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4037 - val_loss: 0.4303\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4007 - val_loss: 0.4262\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3988 - val_loss: 0.4272\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3964 - val_loss: 0.4225\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3936 - val_loss: 0.4204\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3915 - val_loss: 0.4200\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3899 - val_loss: 0.4180\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3874 - val_loss: 0.4170\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3849 - val_loss: 0.4157\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3830 - val_loss: 0.4134\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3816 - val_loss: 0.4127\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3797 - val_loss: 0.4113\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3788 - val_loss: 0.4067\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3764 - val_loss: 0.4060\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3743 - val_loss: 0.4036\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3737 - val_loss: 0.4033\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3713 - val_loss: 0.4015\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3692 - val_loss: 0.4006\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3679 - val_loss: 0.4007\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3670 - val_loss: 0.3969\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3645 - val_loss: 0.3971\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3638 - val_loss: 0.3942\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3612 - val_loss: 0.3928\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3606 - val_loss: 0.3918\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3591 - val_loss: 0.3895\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3571 - val_loss: 0.3897\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3557 - val_loss: 0.3871\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3553 - val_loss: 0.3861\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3529 - val_loss: 0.3871\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3518 - val_loss: 0.3850\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3505 - val_loss: 0.3840\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3490 - val_loss: 0.3850\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3487 - val_loss: 0.3815\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3465 - val_loss: 0.3799\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3455 - val_loss: 0.3781\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3428 - val_loss: 0.3796\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3432 - val_loss: 0.3779\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3412 - val_loss: 0.3757\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3406 - val_loss: 0.3750\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3396 - val_loss: 0.3734\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3382 - val_loss: 0.3747\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3369 - val_loss: 0.3728\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3359 - val_loss: 0.3722\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3349 - val_loss: 0.3701\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3337 - val_loss: 0.3705\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3323 - val_loss: 0.3720\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3320 - val_loss: 0.3694\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3313 - val_loss: 0.3667\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3288 - val_loss: 0.3654\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3293 - val_loss: 0.3654\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3272 - val_loss: 0.3662\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3266 - val_loss: 0.3675\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3262 - val_loss: 0.3632\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3252 - val_loss: 0.3640\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3235 - val_loss: 0.3648\n",
            "Epoch 80/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3237 - val_loss: 0.3648\n",
            "Epoch 81/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3232 - val_loss: 0.3601\n",
            "Epoch 82/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3224 - val_loss: 0.3613\n",
            "Epoch 83/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3212 - val_loss: 0.3591\n",
            "Epoch 84/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3209 - val_loss: 0.3576\n",
            "Epoch 85/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3194 - val_loss: 0.3671\n",
            "Epoch 86/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3190 - val_loss: 0.3576\n",
            "Epoch 87/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3172 - val_loss: 0.3575\n",
            "Epoch 88/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3172 - val_loss: 0.3570\n",
            "Epoch 89/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3160 - val_loss: 0.3579\n",
            "Epoch 90/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3168 - val_loss: 0.3547\n",
            "Epoch 91/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3142 - val_loss: 0.3561\n",
            "Epoch 92/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3139 - val_loss: 0.3535\n",
            "Epoch 93/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3140 - val_loss: 0.3527\n",
            "Epoch 94/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3122 - val_loss: 0.3548\n",
            "Epoch 95/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3117 - val_loss: 0.3526\n",
            "Epoch 96/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3113 - val_loss: 0.3534\n",
            "Epoch 97/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3109 - val_loss: 0.3506\n",
            "Epoch 98/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3092 - val_loss: 0.3500\n",
            "Epoch 99/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3109 - val_loss: 0.3511\n",
            "Epoch 100/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3084 - val_loss: 0.3496\n",
            "121/121 [==============================] - 0s 2ms/step - loss: 0.3365\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "242/242 [==============================] - 1s 3ms/step - loss: 2.0530 - val_loss: 0.8096\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7096 - val_loss: 0.6641\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6265 - val_loss: 0.6213\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5824 - val_loss: 0.5851\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5467 - val_loss: 0.5537\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5180 - val_loss: 0.5272\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4952 - val_loss: 0.5099\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4779 - val_loss: 0.4941\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4633 - val_loss: 0.4806\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4532 - val_loss: 0.4720\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4447 - val_loss: 0.4713\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4389 - val_loss: 0.4614\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4334 - val_loss: 0.4555\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4285 - val_loss: 0.4521\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4237 - val_loss: 0.4470\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4202 - val_loss: 0.4454\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4168 - val_loss: 0.4414\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4134 - val_loss: 0.4391\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4108 - val_loss: 0.4364\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4074 - val_loss: 0.4345\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4041 - val_loss: 0.4317\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4017 - val_loss: 0.4279\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3998 - val_loss: 0.4271\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3969 - val_loss: 0.4236\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3941 - val_loss: 0.4252\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3926 - val_loss: 0.4211\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3898 - val_loss: 0.4186\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3874 - val_loss: 0.4165\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3866 - val_loss: 0.4149\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3828 - val_loss: 0.4146\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3815 - val_loss: 0.4115\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3791 - val_loss: 0.4105\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3781 - val_loss: 0.4101\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3762 - val_loss: 0.4057\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3735 - val_loss: 0.4071\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3722 - val_loss: 0.4024\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3701 - val_loss: 0.4020\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3678 - val_loss: 0.4013\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3690 - val_loss: 0.3991\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3644 - val_loss: 0.3975\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3632 - val_loss: 0.3984\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3618 - val_loss: 0.3960\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3604 - val_loss: 0.3928\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3578 - val_loss: 0.3921\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3577 - val_loss: 0.3915\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3557 - val_loss: 0.3899\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3538 - val_loss: 0.3892\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3543 - val_loss: 0.3872\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3502 - val_loss: 0.3865\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3500 - val_loss: 0.3853\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3479 - val_loss: 0.3845\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3464 - val_loss: 0.3854\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3451 - val_loss: 0.3820\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3440 - val_loss: 0.3785\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3417 - val_loss: 0.3788\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3408 - val_loss: 0.3782\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3410 - val_loss: 0.3769\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3388 - val_loss: 0.3773\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3390 - val_loss: 0.3762\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3375 - val_loss: 0.3757\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3371 - val_loss: 0.3741\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3344 - val_loss: 0.3720\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3341 - val_loss: 0.3740\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3332 - val_loss: 0.3713\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3307 - val_loss: 0.3688\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3302 - val_loss: 0.3696\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3284 - val_loss: 0.3674\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3282 - val_loss: 0.3665\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3269 - val_loss: 0.3689\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3263 - val_loss: 0.3677\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3255 - val_loss: 0.3674\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3237 - val_loss: 0.3664\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3238 - val_loss: 0.3658\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3240 - val_loss: 0.3626\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3248 - val_loss: 0.3630\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3204 - val_loss: 0.3614\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3194 - val_loss: 0.3610\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3175 - val_loss: 0.3644\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3175 - val_loss: 0.3586\n",
            "Epoch 80/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3174 - val_loss: 0.3610\n",
            "Epoch 81/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3154 - val_loss: 0.3636\n",
            "Epoch 82/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3143 - val_loss: 0.3614\n",
            "Epoch 83/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3141 - val_loss: 0.3563\n",
            "Epoch 84/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3120 - val_loss: 0.3569\n",
            "Epoch 85/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3128 - val_loss: 0.3567\n",
            "Epoch 86/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3131 - val_loss: 0.3542\n",
            "Epoch 87/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3104 - val_loss: 0.3545\n",
            "Epoch 88/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3100 - val_loss: 0.3512\n",
            "Epoch 89/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3084 - val_loss: 0.3532\n",
            "Epoch 90/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3088 - val_loss: 0.3500\n",
            "Epoch 91/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3055 - val_loss: 0.3536\n",
            "Epoch 92/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3072 - val_loss: 0.3503\n",
            "Epoch 93/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3070 - val_loss: 0.3498\n",
            "Epoch 94/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3043 - val_loss: 0.3484\n",
            "Epoch 95/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3048 - val_loss: 0.3491\n",
            "Epoch 96/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3047 - val_loss: 0.3459\n",
            "Epoch 97/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3028 - val_loss: 0.3471\n",
            "Epoch 98/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3019 - val_loss: 0.3449\n",
            "Epoch 99/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3012 - val_loss: 0.3475\n",
            "Epoch 100/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3023 - val_loss: 0.3476\n",
            "121/121 [==============================] - 0s 2ms/step - loss: 0.3466\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "242/242 [==============================] - 1s 3ms/step - loss: 1.7531 - val_loss: 0.8349\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.7085 - val_loss: 0.7270\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.6478 - val_loss: 0.6755\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.6127 - val_loss: 0.6377\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.5856 - val_loss: 0.6085\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.5645 - val_loss: 0.5846\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.5474 - val_loss: 0.5664\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.5327 - val_loss: 0.5498\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.5201 - val_loss: 0.5399\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.5102 - val_loss: 0.5269\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.5009 - val_loss: 0.5166\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4923 - val_loss: 0.5077\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4857 - val_loss: 0.5025\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4782 - val_loss: 0.4974\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4727 - val_loss: 0.4913\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4677 - val_loss: 0.4829\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4618 - val_loss: 0.4773\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4570 - val_loss: 0.4731\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4522 - val_loss: 0.4688\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4476 - val_loss: 0.4673\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4439 - val_loss: 0.4608\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4403 - val_loss: 0.4604\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4363 - val_loss: 0.4549\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4330 - val_loss: 0.4514\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4296 - val_loss: 0.4492\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4267 - val_loss: 0.4495\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4237 - val_loss: 0.4455\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4208 - val_loss: 0.4404\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4182 - val_loss: 0.4385\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4154 - val_loss: 0.4393\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4130 - val_loss: 0.4358\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4102 - val_loss: 0.4321\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4085 - val_loss: 0.4315\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.4054 - val_loss: 0.4305\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.4042 - val_loss: 0.4275\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4020 - val_loss: 0.4256\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3998 - val_loss: 0.4238\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3978 - val_loss: 0.4219\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3956 - val_loss: 0.4214\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3937 - val_loss: 0.4192\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3921 - val_loss: 0.4180\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3903 - val_loss: 0.4169\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3880 - val_loss: 0.4134\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3860 - val_loss: 0.4121\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3845 - val_loss: 0.4106\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3830 - val_loss: 0.4137\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3816 - val_loss: 0.4101\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3797 - val_loss: 0.4079\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3779 - val_loss: 0.4080\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3768 - val_loss: 0.4065\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3746 - val_loss: 0.4044\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3733 - val_loss: 0.4030\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3716 - val_loss: 0.4063\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3713 - val_loss: 0.3989\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3689 - val_loss: 0.4014\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3672 - val_loss: 0.3976\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3656 - val_loss: 0.3969\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3644 - val_loss: 0.3957\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3632 - val_loss: 0.3951\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3618 - val_loss: 0.3983\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3607 - val_loss: 0.3932\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3581 - val_loss: 0.3927\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3573 - val_loss: 0.3945\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3565 - val_loss: 0.3890\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3551 - val_loss: 0.3926\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3538 - val_loss: 0.3903\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3522 - val_loss: 0.3888\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3514 - val_loss: 0.3875\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3502 - val_loss: 0.3870\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3493 - val_loss: 0.3857\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3475 - val_loss: 0.3849\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3465 - val_loss: 0.3855\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3465 - val_loss: 0.3834\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3445 - val_loss: 0.3825\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3428 - val_loss: 0.3845\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3422 - val_loss: 0.3811\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3407 - val_loss: 0.3886\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3406 - val_loss: 0.3832\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3400 - val_loss: 0.3802\n",
            "Epoch 80/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3396 - val_loss: 0.3794\n",
            "Epoch 81/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3374 - val_loss: 0.3790\n",
            "Epoch 82/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3349 - val_loss: 0.3873\n",
            "Epoch 83/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3355 - val_loss: 0.3762\n",
            "Epoch 84/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3342 - val_loss: 0.3768\n",
            "Epoch 85/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3332 - val_loss: 0.3756\n",
            "Epoch 86/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3319 - val_loss: 0.3799\n",
            "Epoch 87/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3348 - val_loss: 0.3745\n",
            "Epoch 88/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3304 - val_loss: 0.3734\n",
            "Epoch 89/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3293 - val_loss: 0.3719\n",
            "Epoch 90/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3280 - val_loss: 0.3829\n",
            "Epoch 91/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3282 - val_loss: 0.3712\n",
            "Epoch 92/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3262 - val_loss: 0.3773\n",
            "Epoch 93/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3259 - val_loss: 0.3709\n",
            "Epoch 94/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3249 - val_loss: 0.3708\n",
            "Epoch 95/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3244 - val_loss: 0.3696\n",
            "Epoch 96/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3228 - val_loss: 0.3689\n",
            "Epoch 97/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3220 - val_loss: 0.3705\n",
            "Epoch 98/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3220 - val_loss: 0.3670\n",
            "Epoch 99/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3213 - val_loss: 0.3668\n",
            "Epoch 100/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3198 - val_loss: 0.3698\n",
            "121/121 [==============================] - 0s 2ms/step - loss: 0.4034\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "242/242 [==============================] - 1s 3ms/step - loss: 1.9628 - val_loss: 0.8370\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.7412 - val_loss: 0.6877\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.6417 - val_loss: 0.6361\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.5991 - val_loss: 0.6040\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.5666 - val_loss: 0.5744\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.5427 - val_loss: 0.5530\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.5217 - val_loss: 0.5358\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.5051 - val_loss: 0.5196\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.4914 - val_loss: 0.5070\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4799 - val_loss: 0.4964\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4702 - val_loss: 0.4888\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.4615 - val_loss: 0.4819\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4546 - val_loss: 0.4750\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4477 - val_loss: 0.4698\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4423 - val_loss: 0.4641\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4370 - val_loss: 0.4620\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.4322 - val_loss: 0.4552\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4283 - val_loss: 0.4513\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4250 - val_loss: 0.4505\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.4211 - val_loss: 0.4460\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.4182 - val_loss: 0.4427\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4144 - val_loss: 0.4395\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4112 - val_loss: 0.4363\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.4088 - val_loss: 0.4339\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4063 - val_loss: 0.4335\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4036 - val_loss: 0.4291\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4014 - val_loss: 0.4275\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3985 - val_loss: 0.4258\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3969 - val_loss: 0.4232\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3943 - val_loss: 0.4211\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3921 - val_loss: 0.4197\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3908 - val_loss: 0.4176\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3883 - val_loss: 0.4190\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3863 - val_loss: 0.4150\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3850 - val_loss: 0.4131\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3826 - val_loss: 0.4137\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3809 - val_loss: 0.4107\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3794 - val_loss: 0.4111\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3779 - val_loss: 0.4071\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3759 - val_loss: 0.4064\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3746 - val_loss: 0.4043\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3729 - val_loss: 0.4030\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3714 - val_loss: 0.4009\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3699 - val_loss: 0.3997\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3680 - val_loss: 0.4008\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3669 - val_loss: 0.3998\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3655 - val_loss: 0.3972\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3638 - val_loss: 0.3953\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3621 - val_loss: 0.3968\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3613 - val_loss: 0.3942\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3600 - val_loss: 0.3930\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3590 - val_loss: 0.3912\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3565 - val_loss: 0.3901\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3563 - val_loss: 0.3902\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3550 - val_loss: 0.3880\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3533 - val_loss: 0.3878\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3524 - val_loss: 0.3888\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3517 - val_loss: 0.3869\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3495 - val_loss: 0.3871\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3489 - val_loss: 0.3831\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3482 - val_loss: 0.3829\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3472 - val_loss: 0.3834\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3455 - val_loss: 0.3809\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3449 - val_loss: 0.3796\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3436 - val_loss: 0.3783\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3423 - val_loss: 0.3790\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3419 - val_loss: 0.3768\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3407 - val_loss: 0.3779\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3398 - val_loss: 0.3754\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3386 - val_loss: 0.3752\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3374 - val_loss: 0.3741\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3366 - val_loss: 0.3767\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3363 - val_loss: 0.3745\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3349 - val_loss: 0.3713\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3338 - val_loss: 0.3748\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3335 - val_loss: 0.3693\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3317 - val_loss: 0.3712\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3320 - val_loss: 0.3727\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3310 - val_loss: 0.3676\n",
            "Epoch 80/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3297 - val_loss: 0.3700\n",
            "Epoch 81/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3289 - val_loss: 0.3668\n",
            "Epoch 82/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3287 - val_loss: 0.3668\n",
            "Epoch 83/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3272 - val_loss: 0.3676\n",
            "Epoch 84/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3256 - val_loss: 0.3668\n",
            "Epoch 85/100\n",
            "242/242 [==============================] - 1s 4ms/step - loss: 0.3266 - val_loss: 0.3635\n",
            "Epoch 86/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3246 - val_loss: 0.3667\n",
            "Epoch 87/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3245 - val_loss: 0.3618\n",
            "Epoch 88/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3235 - val_loss: 0.3640\n",
            "Epoch 89/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3229 - val_loss: 0.3608\n",
            "Epoch 90/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3222 - val_loss: 0.3601\n",
            "Epoch 91/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3209 - val_loss: 0.3605\n",
            "Epoch 92/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3206 - val_loss: 0.3603\n",
            "Epoch 93/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3211 - val_loss: 0.3595\n",
            "Epoch 94/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3204 - val_loss: 0.3611\n",
            "Epoch 95/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3198 - val_loss: 0.3581\n",
            "Epoch 96/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3175 - val_loss: 0.3575\n",
            "Epoch 97/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3173 - val_loss: 0.3565\n",
            "Epoch 98/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3162 - val_loss: 0.3550\n",
            "Epoch 99/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3159 - val_loss: 0.3602\n",
            "Epoch 100/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3152 - val_loss: 0.3550\n",
            "121/121 [==============================] - 0s 2ms/step - loss: 0.3441\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "242/242 [==============================] - 1s 3ms/step - loss: 1.9384 - val_loss: 0.9113\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.7455 - val_loss: 0.6769\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.6307 - val_loss: 0.6288\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.5913 - val_loss: 0.5987\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.5648 - val_loss: 0.5755\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.5438 - val_loss: 0.5570\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.5268 - val_loss: 0.5407\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.5115 - val_loss: 0.5275\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4986 - val_loss: 0.5152\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4877 - val_loss: 0.5038\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4772 - val_loss: 0.4951\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4684 - val_loss: 0.4869\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4601 - val_loss: 0.4799\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4524 - val_loss: 0.4751\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4463 - val_loss: 0.4675\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4405 - val_loss: 0.4621\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4347 - val_loss: 0.4574\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4301 - val_loss: 0.4532\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4253 - val_loss: 0.4495\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4213 - val_loss: 0.4463\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4174 - val_loss: 0.4429\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4139 - val_loss: 0.4401\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4112 - val_loss: 0.4369\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4077 - val_loss: 0.4358\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4053 - val_loss: 0.4317\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4022 - val_loss: 0.4297\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.4000 - val_loss: 0.4275\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3977 - val_loss: 0.4251\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3953 - val_loss: 0.4236\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3933 - val_loss: 0.4214\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3905 - val_loss: 0.4189\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3889 - val_loss: 0.4172\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3866 - val_loss: 0.4160\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3848 - val_loss: 0.4143\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3832 - val_loss: 0.4125\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3812 - val_loss: 0.4113\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3796 - val_loss: 0.4105\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3779 - val_loss: 0.4092\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3761 - val_loss: 0.4093\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3745 - val_loss: 0.4054\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3732 - val_loss: 0.4047\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3710 - val_loss: 0.4039\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3702 - val_loss: 0.4020\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3688 - val_loss: 0.4007\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3669 - val_loss: 0.4001\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3657 - val_loss: 0.3991\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3643 - val_loss: 0.3965\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3627 - val_loss: 0.3957\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3619 - val_loss: 0.3971\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3605 - val_loss: 0.3936\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3594 - val_loss: 0.3932\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3572 - val_loss: 0.3915\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3570 - val_loss: 0.3907\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3554 - val_loss: 0.3899\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3545 - val_loss: 0.3882\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3531 - val_loss: 0.3870\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3516 - val_loss: 0.3870\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3505 - val_loss: 0.3860\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3495 - val_loss: 0.3845\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3480 - val_loss: 0.3850\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3468 - val_loss: 0.3838\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3466 - val_loss: 0.3807\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3446 - val_loss: 0.3845\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3439 - val_loss: 0.3805\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3421 - val_loss: 0.3861\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3415 - val_loss: 0.3774\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3405 - val_loss: 0.3779\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3392 - val_loss: 0.3757\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3382 - val_loss: 0.3752\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3373 - val_loss: 0.3741\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3359 - val_loss: 0.3737\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3348 - val_loss: 0.3725\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3341 - val_loss: 0.3739\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3336 - val_loss: 0.3716\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3319 - val_loss: 0.3698\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3312 - val_loss: 0.3702\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3302 - val_loss: 0.3675\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3295 - val_loss: 0.3673\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3276 - val_loss: 0.3658\n",
            "Epoch 80/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3273 - val_loss: 0.3663\n",
            "Epoch 81/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3262 - val_loss: 0.3652\n",
            "Epoch 82/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3257 - val_loss: 0.3649\n",
            "Epoch 83/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3238 - val_loss: 0.3649\n",
            "Epoch 84/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3238 - val_loss: 0.3622\n",
            "Epoch 85/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3223 - val_loss: 0.3612\n",
            "Epoch 86/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3216 - val_loss: 0.3628\n",
            "Epoch 87/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3201 - val_loss: 0.3601\n",
            "Epoch 88/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3199 - val_loss: 0.3609\n",
            "Epoch 89/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3193 - val_loss: 0.3597\n",
            "Epoch 90/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3180 - val_loss: 0.3592\n",
            "Epoch 91/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3175 - val_loss: 0.3577\n",
            "Epoch 92/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3166 - val_loss: 0.3567\n",
            "Epoch 93/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3159 - val_loss: 0.3584\n",
            "Epoch 94/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3149 - val_loss: 0.3555\n",
            "Epoch 95/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3152 - val_loss: 0.3550\n",
            "Epoch 96/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3139 - val_loss: 0.3556\n",
            "Epoch 97/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3130 - val_loss: 0.3545\n",
            "Epoch 98/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3128 - val_loss: 0.3527\n",
            "Epoch 99/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.3115 - val_loss: 0.3525\n",
            "Epoch 100/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3103 - val_loss: 0.3521\n",
            "121/121 [==============================] - 0s 2ms/step - loss: 0.3456\n",
            "Epoch 1/100\n",
            "363/363 [==============================] - 1s 3ms/step - loss: 0.9969 - val_loss: 0.6144\n",
            "Epoch 2/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5596 - val_loss: 0.5423\n",
            "Epoch 3/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4996 - val_loss: 0.4988\n",
            "Epoch 4/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4675 - val_loss: 0.4735\n",
            "Epoch 5/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4444 - val_loss: 0.4553\n",
            "Epoch 6/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4315 - val_loss: 0.4459\n",
            "Epoch 7/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4188 - val_loss: 0.4334\n",
            "Epoch 8/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4034 - val_loss: 0.4259\n",
            "Epoch 9/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3953 - val_loss: 0.4156\n",
            "Epoch 10/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3863 - val_loss: 0.4053\n",
            "Epoch 11/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3797 - val_loss: 0.4019\n",
            "Epoch 12/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3748 - val_loss: 0.3972\n",
            "Epoch 13/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3691 - val_loss: 0.3866\n",
            "Epoch 14/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3642 - val_loss: 0.3874\n",
            "Epoch 15/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3583 - val_loss: 0.3811\n",
            "Epoch 16/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3564 - val_loss: 0.3789\n",
            "Epoch 17/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3574 - val_loss: 0.3793\n",
            "Epoch 18/100\n",
            "363/363 [==============================] - 1s 3ms/step - loss: 0.3557 - val_loss: 0.3709\n",
            "Epoch 19/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3418 - val_loss: 0.3639\n",
            "Epoch 20/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3363 - val_loss: 0.3582\n",
            "Epoch 21/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3337 - val_loss: 0.3540\n",
            "Epoch 22/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3328 - val_loss: 0.3552\n",
            "Epoch 23/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3292 - val_loss: 0.3552\n",
            "Epoch 24/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3254 - val_loss: 0.3560\n",
            "Epoch 25/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3226 - val_loss: 0.3538\n",
            "Epoch 26/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3195 - val_loss: 0.3524\n",
            "Epoch 27/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3170 - val_loss: 0.3451\n",
            "Epoch 28/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3161 - val_loss: 0.3392\n",
            "Epoch 29/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3116 - val_loss: 0.3433\n",
            "Epoch 30/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3102 - val_loss: 0.3408\n",
            "Epoch 31/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3065 - val_loss: 0.3407\n",
            "Epoch 32/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3064 - val_loss: 0.3366\n",
            "Epoch 33/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3030 - val_loss: 0.3344\n",
            "Epoch 34/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3011 - val_loss: 0.3333\n",
            "Epoch 35/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3005 - val_loss: 0.3413\n",
            "Epoch 36/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3062 - val_loss: 0.3319\n",
            "Epoch 37/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2957 - val_loss: 0.3297\n",
            "Epoch 38/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2947 - val_loss: 0.3282\n",
            "Epoch 39/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2931 - val_loss: 0.3264\n",
            "Epoch 40/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2920 - val_loss: 0.3278\n",
            "Epoch 41/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2942 - val_loss: 0.3220\n",
            "Epoch 42/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2899 - val_loss: 0.3381\n",
            "Epoch 43/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2886 - val_loss: 0.3270\n",
            "Epoch 44/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2883 - val_loss: 0.3293\n",
            "Epoch 45/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2871 - val_loss: 0.3334\n",
            "Epoch 46/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2851 - val_loss: 0.3218\n",
            "Epoch 47/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2860 - val_loss: 0.3488\n",
            "Epoch 48/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2882 - val_loss: 0.3209\n",
            "Epoch 49/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2880 - val_loss: 0.3347\n",
            "Epoch 50/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2903 - val_loss: 0.3227\n",
            "Epoch 51/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2849 - val_loss: 0.3250\n",
            "Epoch 52/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2831 - val_loss: 0.3247\n",
            "Epoch 53/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2792 - val_loss: 0.3270\n",
            "Epoch 54/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2795 - val_loss: 0.3161\n",
            "Epoch 55/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2783 - val_loss: 0.3188\n",
            "Epoch 56/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2793 - val_loss: 0.3201\n",
            "Epoch 57/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2764 - val_loss: 0.3223\n",
            "Epoch 58/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2757 - val_loss: 0.3193\n",
            "Epoch 59/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2749 - val_loss: 0.3148\n",
            "Epoch 60/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2752 - val_loss: 0.3150\n",
            "Epoch 61/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2754 - val_loss: 0.3197\n",
            "Epoch 62/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2729 - val_loss: 0.3148\n",
            "Epoch 63/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2716 - val_loss: 0.3305\n",
            "Epoch 64/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2714 - val_loss: 0.3248\n",
            "Epoch 65/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2730 - val_loss: 0.3193\n",
            "Epoch 66/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2727 - val_loss: 0.3153\n",
            "Epoch 67/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2722 - val_loss: 0.3120\n",
            "Epoch 68/100\n",
            "363/363 [==============================] - 1s 3ms/step - loss: 0.2699 - val_loss: 0.3185\n",
            "Epoch 69/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2705 - val_loss: 0.3172\n",
            "Epoch 70/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2696 - val_loss: 0.3110\n",
            "Epoch 71/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2680 - val_loss: 0.3156\n",
            "Epoch 72/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2677 - val_loss: 0.3258\n",
            "Epoch 73/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2663 - val_loss: 0.3133\n",
            "Epoch 74/100\n",
            "363/363 [==============================] - 1s 3ms/step - loss: 0.2678 - val_loss: 0.3117\n",
            "Epoch 75/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2662 - val_loss: 0.3098\n",
            "Epoch 76/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2669 - val_loss: 0.3158\n",
            "Epoch 77/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2658 - val_loss: 0.3212\n",
            "Epoch 78/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2656 - val_loss: 0.3081\n",
            "Epoch 79/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2630 - val_loss: 0.3149\n",
            "Epoch 80/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2634 - val_loss: 0.3162\n",
            "Epoch 81/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2635 - val_loss: 0.3119\n",
            "Epoch 82/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2632 - val_loss: 0.3328\n",
            "Epoch 83/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2623 - val_loss: 0.3096\n",
            "Epoch 84/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2632 - val_loss: 0.3141\n",
            "Epoch 85/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2621 - val_loss: 0.3094\n",
            "Epoch 86/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2601 - val_loss: 0.3095\n",
            "Epoch 87/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2607 - val_loss: 0.3198\n",
            "Epoch 88/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2627 - val_loss: 0.3077\n",
            "Epoch 89/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2608 - val_loss: 0.3100\n",
            "Epoch 90/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2617 - val_loss: 0.3121\n",
            "Epoch 91/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2592 - val_loss: 0.3124\n",
            "Epoch 92/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2596 - val_loss: 0.3115\n",
            "Epoch 93/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2577 - val_loss: 0.3065\n",
            "Epoch 94/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2593 - val_loss: 0.3050\n",
            "Epoch 95/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2589 - val_loss: 0.3083\n",
            "Epoch 96/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2563 - val_loss: 0.3213\n",
            "Epoch 97/100\n",
            "363/363 [==============================] - 1s 3ms/step - loss: 0.2582 - val_loss: 0.3078\n",
            "Epoch 98/100\n",
            "363/363 [==============================] - 1s 3ms/step - loss: 0.2561 - val_loss: 0.3112\n",
            "Epoch 99/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2566 - val_loss: 0.3115\n",
            "Epoch 100/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2561 - val_loss: 0.3054\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=3,\n",
              "                   estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x7fc8a9b93b50>,\n",
              "                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fc8aa835710>,\n",
              "                                        'n_hidden': [0, 1, 2, 3],\n",
              "                                        'n_neurons': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
              "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
              "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
              "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
              "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
              "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])})"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rnd_search_cv.best_params_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enZoRSV5i_eH",
        "outputId": "ad749ae4-e1bd-461a-9e3b-91d1b678398c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'learning_rate': 0.004142809095903494, 'n_hidden': 3, 'n_neurons': 70}"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rnd_search_cv.best_score_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NC9hkwc_jAQe",
        "outputId": "48ff3f6f-cfc4-4ebe-c59f-745fd44eae43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.3273305396238963"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = rnd_search_cv.best_estimator_.model\n",
        "model"
      ],
      "metadata": {
        "id": "WkWjCS-djDQ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44ef67aa-382f-47c9-ba0d-313baf909807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.sequential.Sequential at 0x7fc8a67f71d0>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    }
  ]
}